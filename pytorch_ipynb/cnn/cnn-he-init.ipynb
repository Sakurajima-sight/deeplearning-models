{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.11\n",
      "IPython version      : 9.0.2\n",
      "\n",
      "torch: 2.6.0+cu126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runs on CPU or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Convolutional Neural Network with He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 如果有可用的GPU则使用GPU，否则使用CPU\n",
    "\n",
    "# 超参数设置\n",
    "random_seed = 1            # 随机种子，确保实验可复现\n",
    "learning_rate = 0.05       # 学习率\n",
    "num_epochs = 10            # 训练轮数\n",
    "batch_size = 128           # 每个批次的样本数\n",
    "\n",
    "# 网络架构相关\n",
    "num_classes = 10           # 类别数（MNIST数据集有10个类别：0到9）\n",
    "\n",
    "##########################\n",
    "### MNIST 数据集\n",
    "##########################\n",
    "\n",
    "# 注意：transforms.ToTensor() 会将输入图像的像素值缩放到0到1的范围\n",
    "train_dataset = datasets.MNIST(root='data',           # 数据存储的路径\n",
    "                               train=True,            # 训练集\n",
    "                               transform=transforms.ToTensor(),  # 数据转换：将图像转为Tensor并归一化到[0,1]\n",
    "                               download=True)         # 如果数据集不存在则下载\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data',            # 数据存储的路径\n",
    "                              train=False,           # 测试集\n",
    "                              transform=transforms.ToTensor())  # 数据转换：将图像转为Tensor并归一化到[0,1]\n",
    "\n",
    "# 创建训练数据的DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset,      # 数据集\n",
    "                          batch_size=batch_size,     # 批次大小\n",
    "                          shuffle=True)              # 打乱数据\n",
    "\n",
    "# 创建测试数据的DataLoader\n",
    "test_loader = DataLoader(dataset=test_dataset,        # 数据集\n",
    "                         batch_size=batch_size,      # 批次大小\n",
    "                         shuffle=False)              # 不打乱测试数据\n",
    "\n",
    "# 检查数据集的一些基本信息\n",
    "for images, labels in train_loader:  # 取一个batch的数据\n",
    "    print('Image batch dimensions:', images.shape)  # 打印图像的尺寸\n",
    "    print('Image label dimensions:', labels.shape)  # 打印标签的尺寸\n",
    "    break  # 只查看一个批次的数据，查看数据格式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### 模型定义\n",
    "##########################\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # 计算 same padding（保持输入输出尺寸一致）所需的填充量公式：\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # 推导得：p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        # 卷积层1: 输入为28x28x1 -> 输出为28x28x4\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,           # 输入通道数：灰度图像为1\n",
    "                                      out_channels=4,          # 卷积核数量：输出通道数为4\n",
    "                                      kernel_size=(3, 3),      # 卷积核大小为3x3\n",
    "                                      stride=(1, 1),           # 步幅为1\n",
    "                                      padding=1)               # 填充为1：保持输出尺寸与输入相同\n",
    "                                      # 对应公式：(1(28-1) - 28 + 3)/2 = 1\n",
    "        \n",
    "        # 最大池化层1: 输入为28x28x4 -> 输出为14x14x4\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),   # 池化核大小为2x2\n",
    "                                         stride=(2, 2),         # 步幅为2\n",
    "                                         padding=0)             # 不需要填充：直接缩小尺寸\n",
    "                                         # 输出尺寸变为一半：28 -> 14\n",
    "        \n",
    "        # 卷积层2: 输入为14x14x4 -> 输出为14x14x8\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,           # 输入通道数为4\n",
    "                                      out_channels=8,          # 输出通道数为8\n",
    "                                      kernel_size=(3, 3),      # 卷积核大小为3x3\n",
    "                                      stride=(1, 1),           \n",
    "                                      padding=1)               # 填充为1，保持尺寸\n",
    "                                      # 对应公式：(1(14-1) - 14 + 3)/2 = 1\n",
    "        \n",
    "        # 最大池化层2: 输入为14x14x8 -> 输出为7x7x8\n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0)            # 不需要填充：14 -> 7\n",
    "        \n",
    "        # 全连接层：输入特征数为 7*7*8，输出为类别数（10）\n",
    "        self.linear_1 = torch.nn.Linear(7*7*8, num_classes)\n",
    "        \n",
    "        ###############################################\n",
    "        # 权重初始化：使用 He（Kaiming）初始化方式\n",
    "        ###############################################\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.detach())  # 初始化卷积权重\n",
    "                m.bias.detach().zero_()                     # 初始化偏置为0\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.detach())  # 初始化全连接层权重\n",
    "                m.bias.detach().zero_()                     # 初始化偏置为0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 前向传播过程\n",
    "        \n",
    "        out = self.conv_1(x)        # 卷积层1\n",
    "        out = F.relu(out)           # 激活函数 ReLU\n",
    "        out = self.pool_1(out)      # 最大池化1\n",
    "\n",
    "        out = self.conv_2(out)      # 卷积层2\n",
    "        out = F.relu(out)           # 激活函数 ReLU\n",
    "        out = self.pool_2(out)      # 最大池化2\n",
    "        \n",
    "        # 展平：将7x7x8的特征图展开为一维向量\n",
    "        logits = self.linear_1(out.view(-1, 7*7*8))  # 全连接层\n",
    "        probas = F.softmax(logits, dim=1)            # 输出为各类别的概率（Softmax）\n",
    "        return logits, probas\n",
    "\n",
    "# 设置随机种子，保证结果可复现\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# 创建模型对象，指定类别数（MNIST为10类）\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "\n",
    "# 将模型移动到设备上（GPU 或 CPU）\n",
    "model = model.to(device)\n",
    "\n",
    "# 使用随机梯度下降（SGD）优化器，并设置学习率\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.4756\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 1.1244\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.7610\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.3307\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.4453\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.3692\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.2474\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.2564\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.1947\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.2040\n",
      "Epoch: 001/010 training accuracy: 92.00%\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.2104\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.2966\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.1638\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.1436\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.3372\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.1965\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.2100\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.1994\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.1761\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.1119\n",
      "Epoch: 002/010 training accuracy: 94.99%\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.0716\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.1074\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.1424\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.1596\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.0333\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.2039\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.0849\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.0894\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.1664\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.0576\n",
      "Epoch: 003/010 training accuracy: 96.01%\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.1652\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.0993\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.1714\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.0958\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.1072\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.1009\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.0907\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.0581\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.1445\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.0850\n",
      "Epoch: 004/010 training accuracy: 96.71%\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.1226\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.1128\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.0721\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.1414\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.1223\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.0298\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.2363\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.0972\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.1337\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.1072\n",
      "Epoch: 005/010 training accuracy: 97.13%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.0510\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.0484\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.0898\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.0814\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.1044\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.1145\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.1093\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.1064\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.0203\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.0364\n",
      "Epoch: 006/010 training accuracy: 97.34%\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.0694\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.1007\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.0697\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.0547\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.1079\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.1015\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.1037\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.1806\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.1015\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.0797\n",
      "Epoch: 007/010 training accuracy: 97.68%\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.1319\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.0338\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.0366\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.0835\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.1058\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.1225\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.0561\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.0499\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.1365\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.0442\n",
      "Epoch: 008/010 training accuracy: 97.74%\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.0738\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.0963\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.1377\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.0246\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.0573\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.0790\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.0568\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.0838\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.0469\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.0582\n",
      "Epoch: 009/010 training accuracy: 97.80%\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.0728\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.0220\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.0667\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.0664\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.1252\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.0722\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.0347\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.0515\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.0820\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.0702\n",
      "Epoch: 010/010 training accuracy: 98.08%\n",
      "Time elapsed: 0.51 min\n",
      "Total Training Time: 0.51 min\n"
     ]
    }
   ],
   "source": [
    "# 计算模型在给定数据集（如训练集或测试集）上的准确率\n",
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0  # 正确预测的数量，总样本数\n",
    "    for features, targets in data_loader:\n",
    "        features = features.to(device)  # 将输入数据移动到GPU或CPU\n",
    "        targets = targets.to(device)    # 将目标标签移动到GPU或CPU\n",
    "        logits, probas = model(features)  # 前向传播，得到logits和softmax概率\n",
    "        _, predicted_labels = torch.max(probas, 1)  # 找出每个样本预测概率最大的类别\n",
    "        num_examples += targets.size(0)  # 累加总样本数\n",
    "        correct_pred += (predicted_labels == targets).sum()  # 累加预测正确的数量\n",
    "    return correct_pred.float() / num_examples * 100  # 返回准确率（百分比）\n",
    "\n",
    "\n",
    "# 记录训练起始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 开始训练多个epoch（轮次）\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()  # 设置模型为训练模式（启用dropout、BN等）\n",
    "\n",
    "    # 遍历每一个训练批次\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)  # 将特征张量转移到GPU或CPU\n",
    "        targets = targets.to(device)    # 将标签张量转移到GPU或CPU\n",
    "\n",
    "        ### 前向传播和反向传播\n",
    "        logits, probas = model(features)                   # 前向传播\n",
    "        cost = F.cross_entropy(logits, targets)           # 计算交叉熵损失\n",
    "        optimizer.zero_grad()                             # 梯度清零\n",
    "        \n",
    "        cost.backward()                                   # 反向传播，计算梯度\n",
    "\n",
    "        ### 更新模型参数\n",
    "        optimizer.step()                                  # 用优化器执行一步梯度下降\n",
    "        \n",
    "        ### 日志打印（每50个批次打印一次）\n",
    "        if not batch_idx % 50:\n",
    "            print('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                  % (epoch + 1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "    \n",
    "    model = model.eval()  # 每个epoch结束后将模型设置为评估模式（禁用dropout等）\n",
    "    \n",
    "    # 计算当前epoch在训练集上的准确率\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "          epoch + 1, num_epochs, \n",
    "          compute_accuracy(model, train_loader)))\n",
    "\n",
    "    # 打印本轮训练耗时\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
    "\n",
    "# 打印总训练耗时\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集准确率: 97.90%\n"
     ]
    }
   ],
   "source": [
    "# 打印模型在测试集上的准确率，保留两位小数\n",
    "print('测试集准确率: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision: 0.21.0+cu126\n",
      "numpy      : 1.26.4\n",
      "torch      : 2.6.0+cu126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
