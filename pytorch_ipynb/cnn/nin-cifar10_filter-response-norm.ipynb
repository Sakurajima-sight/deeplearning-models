{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEBilEjLj5wY"
   },
   "source": [
    "Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.\n",
    "- Author: Sebastian Raschka\n",
    "- GitHub Repository: https://github.com/rasbt/deeplearning-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1524974472601,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "GOzuY8Yvj5wb",
    "outputId": "c19362ce-f87a-4cc2-84cc-8d7b4b9e6007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.11\n",
      "IPython version      : 8.30.0\n",
      "\n",
      "torch: 2.6.0+cu126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4XmErYj5wm"
   },
   "source": [
    "# Filter Response Normalization for Network-in-Network CIFAR-10 Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN architecture is based on  \n",
    "CNN架构是基于  \n",
    "\n",
    "- Lin, Min, Qiang Chen, and Shuicheng Yan. \"[Network in network](https://arxiv.org/abs/1312.4400).\" arXiv preprint arXiv:1312.4400 (2013).  \n",
    "- 林敏，陈强，颜水成。\"[网络中的网络](https://arxiv.org/abs/1312.4400)\"。arXiv预印本 arXiv:1312.4400（2013年）。  \n",
    "\n",
    "This notebook implements Filter Response Normalization as a drop-in replacement for BatchNorm, based on the paper:  \n",
    "本笔记本实现了过滤响应归一化，作为BatchNorm的替代方案，基于以下论文：  \n",
    "\n",
    "- S. Singh and S. Krishnan (2019). **Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks.** https://arxiv.org/abs/1911.09737  \n",
    "- S. Singh 和 S. Krishnan（2019年）。**过滤响应归一化层：消除深度神经网络训练中的批次依赖性。** https://arxiv.org/abs/1911.09737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FilterResponseNormalization(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-6):\n",
    "        super(FilterResponseNormalization, self).__init__()\n",
    "        \n",
    "        # 注册可训练参数beta，初始化为服从正态分布的张量\n",
    "        self.register_parameter('beta', \n",
    "                                torch.nn.Parameter(\n",
    "                                        torch.empty([1, num_features, 1, 1]).normal_()))\n",
    "    \n",
    "        # 注册可训练参数gamma，初始化为服从正态分布的张量\n",
    "        self.register_parameter('gamma', \n",
    "                                torch.nn.Parameter(\n",
    "                                        torch.empty([1, num_features, 1, 1]).normal_()))\n",
    "        \n",
    "        # 注册可训练参数tau，初始化为服从正态分布的张量\n",
    "        self.register_parameter('tau', \n",
    "                                torch.nn.Parameter(\n",
    "                                        torch.empty([1, num_features, 1, 1]).normal_()))\n",
    "        \n",
    "        self.eps = torch.Tensor([eps])  # 初始化eps，防止除零错误\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数，参考了\n",
    "        # https://github.com/gupta-abhay/pytorch-frn/blob/master/frn.py\n",
    "        n, c, h, w = x.size()  # 获取输入张量的尺寸\n",
    "        \n",
    "        self.eps = self.eps.to(self.tau.device)  # 确保eps在与tau相同的设备上\n",
    "\n",
    "        # 计算输入张量x的平方平均值（通道维度上的均值）\n",
    "        nu2 = torch.mean(x.pow(2), (2, 3), keepdims=True)\n",
    "        \n",
    "        # 对输入张量x进行标准化，利用平方均值和eps\n",
    "        x = x * torch.rsqrt(nu2 + torch.abs(self.eps))\n",
    "        \n",
    "        # 返回经过FRN归一化处理后的结果，包含gamma、beta和tau的影响\n",
    "        return torch.max(self.gamma * x + self.beta, self.tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkoGLH_Tj5wn"
   },
   "source": [
    "## Additional Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.set_per_process_memory_fraction(0.5, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### 设置\n",
    "##########################\n",
    "\n",
    "# 超参数\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# 网络架构\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# 其他设置\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集图像批次的维度: torch.Size([256, 3, 32, 32])\n",
      "训练集标签批次的维度: torch.Size([256])\n",
      "测试集图像批次的维度: torch.Size([256, 3, 32, 32])\n",
      "测试集标签批次的维度: torch.Size([256])\n",
      "验证集图像批次的维度: torch.Size([256, 3, 32, 32])\n",
      "验证集标签批次的维度: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### CIFAR-10 数据集\n",
    "##########################\n",
    "\n",
    "# 注意 transforms.ToTensor() 会将输入图像缩放到 0-1 范围内\n",
    "\n",
    "train_indices = torch.arange(0, 49000)  # 训练集的索引\n",
    "valid_indices = torch.arange(49000, 50000)  # 验证集的索引\n",
    "\n",
    "# 加载 CIFAR-10 数据集，train=True 表示训练集，transform=transforms.ToTensor() 将图像转为 Tensor 格式，下载数据集\n",
    "train_and_valid = datasets.CIFAR10(root='data', \n",
    "                                   train=True, \n",
    "                                   transform=transforms.ToTensor(),\n",
    "                                   download=True)\n",
    "\n",
    "# 从 CIFAR-10 数据集中提取训练集和验证集\n",
    "train_dataset = Subset(train_and_valid, train_indices)\n",
    "valid_dataset = Subset(train_and_valid, valid_indices)\n",
    "\n",
    "# 加载测试集，train=False 表示测试集，transform=transforms.ToTensor() 将图像转为 Tensor 格式\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "#####################################################\n",
    "### 数据加载器\n",
    "#####################################################\n",
    "\n",
    "# 创建训练数据加载器，batch_size 为批次大小，num_workers 设置为 8 表示使用 8 个子进程加载数据，shuffle=True 表示打乱数据\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "# 创建验证数据加载器，shuffle=False 表示不打乱验证集数据\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=8,\n",
    "                          shuffle=False)\n",
    "\n",
    "# 创建测试数据加载器，shuffle=False 表示不打乱测试集数据\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# 检查数据集的形状\n",
    "for images, labels in train_loader:  \n",
    "    print('训练集图像批次的维度:', images.shape)  # 输出训练集图像的维度\n",
    "    print('训练集标签批次的维度:', labels.shape)  # 输出训练集标签的维度\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:  \n",
    "    print('测试集图像批次的维度:', images.shape)  # 输出测试集图像的维度\n",
    "    print('测试集标签批次的维度:', labels.shape)  # 输出测试集标签的维度\n",
    "    break\n",
    "    \n",
    "for images, labels in valid_loader:  \n",
    "    print('验证集图像批次的维度:', images.shape)  # 输出验证集图像的维度\n",
    "    print('验证集标签批次的维度:', labels.shape)  # 输出验证集标签的维度\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Response Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### 模型\n",
    "##########################\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NiN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # 定义网络的分类器部分\n",
    "        self.classifier = nn.Sequential(\n",
    "                # 第一层卷积层：输入通道为 3，输出通道为 192，卷积核大小为 5，步幅为 1，padding 为 2，bias=False 不使用偏置\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "                FilterResponseNormalization(192),  # 使用滤波响应归一化（FRN）进行标准化\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 第二层卷积层：输入通道为 192，输出通道为 160，卷积核大小为 1，步幅为 1，padding 为 0，bias=False 不使用偏置\n",
    "                nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                FilterResponseNormalization(160),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 第三层卷积层：输入通道为 160，输出通道为 96，卷积核大小为 1，步幅为 1，padding 为 0，bias=False 不使用偏置\n",
    "                nn.Conv2d(160,  96, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                FilterResponseNormalization(96),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 最大池化层：卷积后的输出通过最大池化层进行下采样，卷积核大小为 3，步幅为 2，padding 为 1\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                \n",
    "                # Dropout 层：在训练时随机丢弃部分神经元，防止过拟合\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                # 第四层卷积层：输入通道为 96，输出通道为 192，卷积核大小为 5，步幅为 1，padding 为 2，bias=False 不使用偏置\n",
    "                nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "                FilterResponseNormalization(192),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 第五层卷积层：输入通道为 192，输出通道为 192，卷积核大小为 1，步幅为 1，padding 为 0，bias=False 不使用偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                FilterResponseNormalization(192),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 第六层卷积层：输入通道为 192，输出通道为 192，卷积核大小为 1，步幅为 1，padding 为 0，bias=False 不使用偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                FilterResponseNormalization(192),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 平均池化层：卷积后的输出通过平均池化层进行下采样，卷积核大小为 3，步幅为 2，padding 为 1\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                \n",
    "                # Dropout 层：在训练时随机丢弃部分神经元，防止过拟合\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                # 第七层卷积层：输入通道为 192，输出通道为 192，卷积核大小为 3，步幅为 1，padding 为 1，bias=False 不使用偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                FilterResponseNormalization(192),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 第八层卷积层：输入通道为 192，输出通道为 192，卷积核大小为 1，步幅为 1，padding 为 0，bias=False 不使用偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                FilterResponseNormalization(192),  # 使用滤波响应归一化（FRN）\n",
    "                #nn.ReLU(inplace=True),  # ReLU 激活函数已注释，不使用激活函数\n",
    "                \n",
    "                # 第九层卷积层：输入通道为 192，输出通道为 10，卷积核大小为 1，步幅为 1，padding 为 0\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),  # 激活函数 ReLU\n",
    "                \n",
    "                # 平均池化层：卷积后的输出通过平均池化层进行下采样，卷积核大小为 8，步幅为 1，padding 为 0\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播过程\n",
    "        x = self.classifier(x)  # 通过分类器进行计算\n",
    "        logits = x.view(x.size(0), self.num_classes)  # 将输出展平，形状为 [batch_size, num_classes]\n",
    "        probas = torch.softmax(logits, dim=1)  # 使用 softmax 激活函数获得每个类的概率分布\n",
    "        return logits, probas  # 返回 logits 和概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = NiN(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/192 | Cost: 2.3031\n",
      "Epoch: 001/100 | Batch 120/192 | Cost: 2.2107\n",
      "Epoch: 001/100 Train Acc.: 23.16% | Validation Acc.: 26.60%\n",
      "Time elapsed: 1.82 min\n",
      "Epoch: 002/100 | Batch 000/192 | Cost: 2.1562\n",
      "Epoch: 002/100 | Batch 120/192 | Cost: 2.0779\n",
      "Epoch: 002/100 Train Acc.: 24.90% | Validation Acc.: 28.30%\n",
      "Time elapsed: 3.63 min\n",
      "Epoch: 003/100 | Batch 000/192 | Cost: 2.1025\n",
      "Epoch: 003/100 | Batch 120/192 | Cost: 2.0373\n",
      "Epoch: 003/100 Train Acc.: 29.09% | Validation Acc.: 32.20%\n",
      "Time elapsed: 4.83 min\n",
      "Epoch: 004/100 | Batch 000/192 | Cost: 1.9805\n",
      "Epoch: 004/100 | Batch 120/192 | Cost: 1.9233\n",
      "Epoch: 004/100 Train Acc.: 31.22% | Validation Acc.: 32.90%\n",
      "Time elapsed: 5.56 min\n",
      "Epoch: 005/100 | Batch 000/192 | Cost: 1.9335\n",
      "Epoch: 005/100 | Batch 120/192 | Cost: 1.8953\n",
      "Epoch: 005/100 Train Acc.: 35.07% | Validation Acc.: 36.00%\n",
      "Time elapsed: 6.28 min\n",
      "Epoch: 006/100 | Batch 000/192 | Cost: 1.8593\n",
      "Epoch: 006/100 | Batch 120/192 | Cost: 1.8514\n",
      "Epoch: 006/100 Train Acc.: 37.11% | Validation Acc.: 37.40%\n",
      "Time elapsed: 7.01 min\n",
      "Epoch: 007/100 | Batch 000/192 | Cost: 1.7816\n",
      "Epoch: 007/100 | Batch 120/192 | Cost: 1.8437\n",
      "Epoch: 007/100 Train Acc.: 37.75% | Validation Acc.: 37.30%\n",
      "Time elapsed: 7.74 min\n",
      "Epoch: 008/100 | Batch 000/192 | Cost: 1.8311\n",
      "Epoch: 008/100 | Batch 120/192 | Cost: 1.7314\n",
      "Epoch: 008/100 Train Acc.: 39.54% | Validation Acc.: 38.60%\n",
      "Time elapsed: 8.47 min\n",
      "Epoch: 009/100 | Batch 000/192 | Cost: 1.6986\n",
      "Epoch: 009/100 | Batch 120/192 | Cost: 1.8514\n",
      "Epoch: 009/100 Train Acc.: 40.70% | Validation Acc.: 41.20%\n",
      "Time elapsed: 9.19 min\n",
      "Epoch: 010/100 | Batch 000/192 | Cost: 1.7696\n",
      "Epoch: 010/100 | Batch 120/192 | Cost: 1.8028\n",
      "Epoch: 010/100 Train Acc.: 41.40% | Validation Acc.: 42.20%\n",
      "Time elapsed: 9.92 min\n",
      "Epoch: 011/100 | Batch 000/192 | Cost: 1.7444\n",
      "Epoch: 011/100 | Batch 120/192 | Cost: 1.6682\n",
      "Epoch: 011/100 Train Acc.: 42.41% | Validation Acc.: 43.50%\n",
      "Time elapsed: 10.65 min\n",
      "Epoch: 012/100 | Batch 000/192 | Cost: 1.7462\n",
      "Epoch: 012/100 | Batch 120/192 | Cost: 1.7045\n",
      "Epoch: 012/100 Train Acc.: 43.73% | Validation Acc.: 44.40%\n",
      "Time elapsed: 11.38 min\n",
      "Epoch: 013/100 | Batch 000/192 | Cost: 1.6343\n",
      "Epoch: 013/100 | Batch 120/192 | Cost: 1.6024\n",
      "Epoch: 013/100 Train Acc.: 44.59% | Validation Acc.: 45.80%\n",
      "Time elapsed: 12.10 min\n",
      "Epoch: 014/100 | Batch 000/192 | Cost: 1.7637\n",
      "Epoch: 014/100 | Batch 120/192 | Cost: 1.7090\n",
      "Epoch: 014/100 Train Acc.: 45.52% | Validation Acc.: 46.10%\n",
      "Time elapsed: 12.83 min\n",
      "Epoch: 015/100 | Batch 000/192 | Cost: 1.7757\n",
      "Epoch: 015/100 | Batch 120/192 | Cost: 1.5907\n",
      "Epoch: 015/100 Train Acc.: 45.20% | Validation Acc.: 44.70%\n",
      "Time elapsed: 13.56 min\n",
      "Epoch: 016/100 | Batch 000/192 | Cost: 1.4826\n",
      "Epoch: 016/100 | Batch 120/192 | Cost: 1.6057\n",
      "Epoch: 016/100 Train Acc.: 48.94% | Validation Acc.: 50.30%\n",
      "Time elapsed: 14.28 min\n",
      "Epoch: 017/100 | Batch 000/192 | Cost: 1.5494\n",
      "Epoch: 017/100 | Batch 120/192 | Cost: 1.5774\n",
      "Epoch: 017/100 Train Acc.: 48.18% | Validation Acc.: 49.40%\n",
      "Time elapsed: 15.01 min\n",
      "Epoch: 018/100 | Batch 000/192 | Cost: 1.5120\n",
      "Epoch: 018/100 | Batch 120/192 | Cost: 1.4262\n",
      "Epoch: 018/100 Train Acc.: 49.29% | Validation Acc.: 49.00%\n",
      "Time elapsed: 15.74 min\n",
      "Epoch: 019/100 | Batch 000/192 | Cost: 1.4889\n",
      "Epoch: 019/100 | Batch 120/192 | Cost: 1.4012\n",
      "Epoch: 019/100 Train Acc.: 51.05% | Validation Acc.: 51.80%\n",
      "Time elapsed: 16.46 min\n",
      "Epoch: 020/100 | Batch 000/192 | Cost: 1.4777\n",
      "Epoch: 020/100 | Batch 120/192 | Cost: 1.4548\n",
      "Epoch: 020/100 Train Acc.: 51.46% | Validation Acc.: 52.40%\n",
      "Time elapsed: 17.19 min\n",
      "Epoch: 021/100 | Batch 000/192 | Cost: 1.3823\n",
      "Epoch: 021/100 | Batch 120/192 | Cost: 1.4662\n",
      "Epoch: 021/100 Train Acc.: 52.01% | Validation Acc.: 52.00%\n",
      "Time elapsed: 17.92 min\n",
      "Epoch: 022/100 | Batch 000/192 | Cost: 1.4092\n",
      "Epoch: 022/100 | Batch 120/192 | Cost: 1.4513\n",
      "Epoch: 022/100 Train Acc.: 53.07% | Validation Acc.: 53.10%\n",
      "Time elapsed: 18.65 min\n",
      "Epoch: 023/100 | Batch 000/192 | Cost: 1.3682\n",
      "Epoch: 023/100 | Batch 120/192 | Cost: 1.3921\n",
      "Epoch: 023/100 Train Acc.: 53.26% | Validation Acc.: 53.80%\n",
      "Time elapsed: 19.78 min\n",
      "Epoch: 024/100 | Batch 000/192 | Cost: 1.4368\n",
      "Epoch: 024/100 | Batch 120/192 | Cost: 1.4076\n",
      "Epoch: 024/100 Train Acc.: 53.83% | Validation Acc.: 52.60%\n",
      "Time elapsed: 21.59 min\n",
      "Epoch: 025/100 | Batch 000/192 | Cost: 1.3499\n",
      "Epoch: 025/100 | Batch 120/192 | Cost: 1.2854\n",
      "Epoch: 025/100 Train Acc.: 52.67% | Validation Acc.: 54.60%\n",
      "Time elapsed: 23.39 min\n",
      "Epoch: 026/100 | Batch 000/192 | Cost: 1.3559\n",
      "Epoch: 026/100 | Batch 120/192 | Cost: 1.2498\n",
      "Epoch: 026/100 Train Acc.: 54.50% | Validation Acc.: 55.00%\n",
      "Time elapsed: 25.20 min\n",
      "Epoch: 027/100 | Batch 000/192 | Cost: 1.3169\n",
      "Epoch: 027/100 | Batch 120/192 | Cost: 1.1975\n",
      "Epoch: 027/100 Train Acc.: 52.86% | Validation Acc.: 54.20%\n",
      "Time elapsed: 27.00 min\n",
      "Epoch: 028/100 | Batch 000/192 | Cost: 1.2162\n",
      "Epoch: 028/100 | Batch 120/192 | Cost: 1.2408\n",
      "Epoch: 028/100 Train Acc.: 55.93% | Validation Acc.: 55.00%\n",
      "Time elapsed: 28.80 min\n",
      "Epoch: 029/100 | Batch 000/192 | Cost: 1.2840\n",
      "Epoch: 029/100 | Batch 120/192 | Cost: 1.2895\n",
      "Epoch: 029/100 Train Acc.: 56.42% | Validation Acc.: 55.20%\n",
      "Time elapsed: 30.61 min\n",
      "Epoch: 030/100 | Batch 000/192 | Cost: 1.3270\n",
      "Epoch: 030/100 | Batch 120/192 | Cost: 1.4410\n",
      "Epoch: 030/100 Train Acc.: 54.88% | Validation Acc.: 54.30%\n",
      "Time elapsed: 32.42 min\n",
      "Epoch: 031/100 | Batch 000/192 | Cost: 1.3157\n",
      "Epoch: 031/100 | Batch 120/192 | Cost: 1.3024\n",
      "Epoch: 031/100 Train Acc.: 55.24% | Validation Acc.: 54.40%\n",
      "Time elapsed: 34.23 min\n",
      "Epoch: 032/100 | Batch 000/192 | Cost: 1.2602\n",
      "Epoch: 032/100 | Batch 120/192 | Cost: 1.3496\n",
      "Epoch: 032/100 Train Acc.: 56.62% | Validation Acc.: 55.80%\n",
      "Time elapsed: 36.03 min\n",
      "Epoch: 033/100 | Batch 000/192 | Cost: 1.1842\n",
      "Epoch: 033/100 | Batch 120/192 | Cost: 1.2254\n",
      "Epoch: 033/100 Train Acc.: 57.70% | Validation Acc.: 56.70%\n",
      "Time elapsed: 37.84 min\n",
      "Epoch: 034/100 | Batch 000/192 | Cost: 1.2049\n",
      "Epoch: 034/100 | Batch 120/192 | Cost: 1.1483\n",
      "Epoch: 034/100 Train Acc.: 57.19% | Validation Acc.: 56.80%\n",
      "Time elapsed: 39.64 min\n",
      "Epoch: 035/100 | Batch 000/192 | Cost: 1.2248\n",
      "Epoch: 035/100 | Batch 120/192 | Cost: 1.2980\n",
      "Epoch: 035/100 Train Acc.: 57.91% | Validation Acc.: 56.80%\n",
      "Time elapsed: 41.45 min\n",
      "Epoch: 036/100 | Batch 000/192 | Cost: 1.1688\n",
      "Epoch: 036/100 | Batch 120/192 | Cost: 1.2365\n",
      "Epoch: 036/100 Train Acc.: 57.63% | Validation Acc.: 57.10%\n",
      "Time elapsed: 43.25 min\n",
      "Epoch: 037/100 | Batch 000/192 | Cost: 1.2123\n",
      "Epoch: 037/100 | Batch 120/192 | Cost: 1.2266\n",
      "Epoch: 037/100 Train Acc.: 57.85% | Validation Acc.: 55.10%\n",
      "Time elapsed: 45.06 min\n",
      "Epoch: 038/100 | Batch 000/192 | Cost: 1.1669\n",
      "Epoch: 038/100 | Batch 120/192 | Cost: 1.1721\n",
      "Epoch: 038/100 Train Acc.: 57.45% | Validation Acc.: 54.80%\n",
      "Time elapsed: 46.87 min\n",
      "Epoch: 039/100 | Batch 000/192 | Cost: 1.3232\n",
      "Epoch: 039/100 | Batch 120/192 | Cost: 1.2773\n",
      "Epoch: 039/100 Train Acc.: 58.36% | Validation Acc.: 57.10%\n",
      "Time elapsed: 48.67 min\n",
      "Epoch: 040/100 | Batch 000/192 | Cost: 1.1441\n",
      "Epoch: 040/100 | Batch 120/192 | Cost: 1.1045\n",
      "Epoch: 040/100 Train Acc.: 57.25% | Validation Acc.: 54.90%\n",
      "Time elapsed: 50.48 min\n",
      "Epoch: 041/100 | Batch 000/192 | Cost: 1.2291\n",
      "Epoch: 041/100 | Batch 120/192 | Cost: 1.1662\n",
      "Epoch: 041/100 Train Acc.: 59.83% | Validation Acc.: 58.40%\n",
      "Time elapsed: 52.28 min\n",
      "Epoch: 042/100 | Batch 000/192 | Cost: 0.9850\n",
      "Epoch: 042/100 | Batch 120/192 | Cost: 1.1311\n",
      "Epoch: 042/100 Train Acc.: 58.80% | Validation Acc.: 55.80%\n",
      "Time elapsed: 54.09 min\n",
      "Epoch: 043/100 | Batch 000/192 | Cost: 1.0543\n",
      "Epoch: 043/100 | Batch 120/192 | Cost: 1.1915\n",
      "Epoch: 043/100 Train Acc.: 60.32% | Validation Acc.: 58.30%\n",
      "Time elapsed: 55.90 min\n",
      "Epoch: 044/100 | Batch 000/192 | Cost: 1.1088\n",
      "Epoch: 044/100 | Batch 120/192 | Cost: 1.1002\n",
      "Epoch: 044/100 Train Acc.: 61.09% | Validation Acc.: 60.70%\n",
      "Time elapsed: 57.70 min\n",
      "Epoch: 045/100 | Batch 000/192 | Cost: 1.0847\n",
      "Epoch: 045/100 | Batch 120/192 | Cost: 1.0454\n",
      "Epoch: 045/100 Train Acc.: 60.71% | Validation Acc.: 59.80%\n",
      "Time elapsed: 59.51 min\n",
      "Epoch: 046/100 | Batch 000/192 | Cost: 1.0613\n",
      "Epoch: 046/100 | Batch 120/192 | Cost: 1.1056\n",
      "Epoch: 046/100 Train Acc.: 61.10% | Validation Acc.: 58.20%\n",
      "Time elapsed: 61.31 min\n",
      "Epoch: 047/100 | Batch 000/192 | Cost: 1.1532\n",
      "Epoch: 047/100 | Batch 120/192 | Cost: 1.0899\n",
      "Epoch: 047/100 Train Acc.: 61.39% | Validation Acc.: 58.80%\n",
      "Time elapsed: 63.12 min\n",
      "Epoch: 048/100 | Batch 000/192 | Cost: 1.2443\n",
      "Epoch: 048/100 | Batch 120/192 | Cost: 1.1001\n",
      "Epoch: 048/100 Train Acc.: 61.26% | Validation Acc.: 58.70%\n",
      "Time elapsed: 64.92 min\n",
      "Epoch: 049/100 | Batch 000/192 | Cost: 1.1428\n",
      "Epoch: 049/100 | Batch 120/192 | Cost: 1.1397\n",
      "Epoch: 049/100 Train Acc.: 60.81% | Validation Acc.: 57.90%\n",
      "Time elapsed: 66.73 min\n",
      "Epoch: 050/100 | Batch 000/192 | Cost: 1.1206\n",
      "Epoch: 050/100 | Batch 120/192 | Cost: 0.9958\n",
      "Epoch: 050/100 Train Acc.: 61.40% | Validation Acc.: 57.10%\n",
      "Time elapsed: 68.54 min\n",
      "Epoch: 051/100 | Batch 000/192 | Cost: 0.9846\n",
      "Epoch: 051/100 | Batch 120/192 | Cost: 1.0531\n",
      "Epoch: 051/100 Train Acc.: 61.45% | Validation Acc.: 60.10%\n",
      "Time elapsed: 70.34 min\n",
      "Epoch: 052/100 | Batch 000/192 | Cost: 1.1348\n",
      "Epoch: 052/100 | Batch 120/192 | Cost: 1.1228\n",
      "Epoch: 052/100 Train Acc.: 62.36% | Validation Acc.: 61.50%\n",
      "Time elapsed: 72.15 min\n",
      "Epoch: 053/100 | Batch 000/192 | Cost: 1.0542\n",
      "Epoch: 053/100 | Batch 120/192 | Cost: 1.1265\n",
      "Epoch: 053/100 Train Acc.: 62.24% | Validation Acc.: 59.60%\n",
      "Time elapsed: 73.95 min\n",
      "Epoch: 054/100 | Batch 000/192 | Cost: 1.2676\n",
      "Epoch: 054/100 | Batch 120/192 | Cost: 1.1019\n",
      "Epoch: 054/100 Train Acc.: 62.33% | Validation Acc.: 60.60%\n",
      "Time elapsed: 75.76 min\n",
      "Epoch: 055/100 | Batch 000/192 | Cost: 1.0747\n",
      "Epoch: 055/100 | Batch 120/192 | Cost: 1.1667\n",
      "Epoch: 055/100 Train Acc.: 62.89% | Validation Acc.: 61.20%\n",
      "Time elapsed: 77.56 min\n",
      "Epoch: 056/100 | Batch 000/192 | Cost: 1.0925\n",
      "Epoch: 056/100 | Batch 120/192 | Cost: 0.9739\n",
      "Epoch: 056/100 Train Acc.: 62.83% | Validation Acc.: 60.50%\n",
      "Time elapsed: 79.37 min\n",
      "Epoch: 057/100 | Batch 000/192 | Cost: 1.0453\n",
      "Epoch: 057/100 | Batch 120/192 | Cost: 1.0903\n",
      "Epoch: 057/100 Train Acc.: 62.60% | Validation Acc.: 60.20%\n",
      "Time elapsed: 81.17 min\n",
      "Epoch: 058/100 | Batch 000/192 | Cost: 1.0545\n",
      "Epoch: 058/100 | Batch 120/192 | Cost: 1.0492\n",
      "Epoch: 058/100 Train Acc.: 63.84% | Validation Acc.: 62.40%\n",
      "Time elapsed: 82.98 min\n",
      "Epoch: 059/100 | Batch 000/192 | Cost: 1.1170\n",
      "Epoch: 059/100 | Batch 120/192 | Cost: 1.1109\n",
      "Epoch: 059/100 Train Acc.: 62.94% | Validation Acc.: 60.90%\n",
      "Time elapsed: 84.79 min\n",
      "Epoch: 060/100 | Batch 000/192 | Cost: 1.1208\n",
      "Epoch: 060/100 | Batch 120/192 | Cost: 1.1049\n",
      "Epoch: 060/100 Train Acc.: 63.78% | Validation Acc.: 61.30%\n",
      "Time elapsed: 86.59 min\n",
      "Epoch: 061/100 | Batch 000/192 | Cost: 1.1416\n",
      "Epoch: 061/100 | Batch 120/192 | Cost: 1.0359\n",
      "Epoch: 061/100 Train Acc.: 62.77% | Validation Acc.: 59.70%\n",
      "Time elapsed: 88.40 min\n",
      "Epoch: 062/100 | Batch 000/192 | Cost: 1.1623\n",
      "Epoch: 062/100 | Batch 120/192 | Cost: 1.1139\n",
      "Epoch: 062/100 Train Acc.: 62.83% | Validation Acc.: 60.90%\n",
      "Time elapsed: 90.21 min\n",
      "Epoch: 063/100 | Batch 000/192 | Cost: 1.0643\n",
      "Epoch: 063/100 | Batch 120/192 | Cost: 0.9790\n",
      "Epoch: 063/100 Train Acc.: 62.93% | Validation Acc.: 61.60%\n",
      "Time elapsed: 92.02 min\n",
      "Epoch: 064/100 | Batch 000/192 | Cost: 1.0035\n",
      "Epoch: 064/100 | Batch 120/192 | Cost: 1.0436\n",
      "Epoch: 064/100 Train Acc.: 64.56% | Validation Acc.: 62.50%\n",
      "Time elapsed: 93.83 min\n",
      "Epoch: 065/100 | Batch 000/192 | Cost: 1.1809\n",
      "Epoch: 065/100 | Batch 120/192 | Cost: 1.0364\n",
      "Epoch: 065/100 Train Acc.: 64.43% | Validation Acc.: 61.30%\n",
      "Time elapsed: 95.65 min\n",
      "Epoch: 066/100 | Batch 000/192 | Cost: 1.0949\n",
      "Epoch: 066/100 | Batch 120/192 | Cost: 1.1473\n",
      "Epoch: 066/100 Train Acc.: 64.60% | Validation Acc.: 61.40%\n",
      "Time elapsed: 97.46 min\n",
      "Epoch: 067/100 | Batch 000/192 | Cost: 1.0111\n",
      "Epoch: 067/100 | Batch 120/192 | Cost: 1.0604\n",
      "Epoch: 067/100 Train Acc.: 64.18% | Validation Acc.: 61.80%\n",
      "Time elapsed: 99.27 min\n",
      "Epoch: 068/100 | Batch 000/192 | Cost: 1.1092\n",
      "Epoch: 068/100 | Batch 120/192 | Cost: 0.9766\n",
      "Epoch: 068/100 Train Acc.: 64.63% | Validation Acc.: 61.10%\n",
      "Time elapsed: 101.08 min\n",
      "Epoch: 069/100 | Batch 000/192 | Cost: 0.9074\n",
      "Epoch: 069/100 | Batch 120/192 | Cost: 0.9979\n",
      "Epoch: 069/100 Train Acc.: 65.19% | Validation Acc.: 62.10%\n",
      "Time elapsed: 102.90 min\n",
      "Epoch: 070/100 | Batch 000/192 | Cost: 1.0533\n",
      "Epoch: 070/100 | Batch 120/192 | Cost: 1.1004\n",
      "Epoch: 070/100 Train Acc.: 65.31% | Validation Acc.: 62.40%\n",
      "Time elapsed: 104.71 min\n",
      "Epoch: 071/100 | Batch 000/192 | Cost: 1.0430\n",
      "Epoch: 071/100 | Batch 120/192 | Cost: 1.0496\n",
      "Epoch: 071/100 Train Acc.: 65.05% | Validation Acc.: 63.50%\n",
      "Time elapsed: 106.52 min\n",
      "Epoch: 072/100 | Batch 000/192 | Cost: 1.0539\n",
      "Epoch: 072/100 | Batch 120/192 | Cost: 0.9714\n",
      "Epoch: 072/100 Train Acc.: 65.47% | Validation Acc.: 61.80%\n",
      "Time elapsed: 108.34 min\n",
      "Epoch: 073/100 | Batch 000/192 | Cost: 0.9389\n",
      "Epoch: 073/100 | Batch 120/192 | Cost: 0.9766\n",
      "Epoch: 073/100 Train Acc.: 64.91% | Validation Acc.: 62.50%\n",
      "Time elapsed: 110.15 min\n",
      "Epoch: 074/100 | Batch 000/192 | Cost: 0.9319\n",
      "Epoch: 074/100 | Batch 120/192 | Cost: 0.9904\n",
      "Epoch: 074/100 Train Acc.: 64.69% | Validation Acc.: 62.70%\n",
      "Time elapsed: 111.96 min\n",
      "Epoch: 075/100 | Batch 000/192 | Cost: 0.9796\n",
      "Epoch: 075/100 | Batch 120/192 | Cost: 1.0256\n",
      "Epoch: 075/100 Train Acc.: 65.79% | Validation Acc.: 62.90%\n",
      "Time elapsed: 113.77 min\n",
      "Epoch: 076/100 | Batch 000/192 | Cost: 0.9460\n",
      "Epoch: 076/100 | Batch 120/192 | Cost: 0.8705\n",
      "Epoch: 076/100 Train Acc.: 65.60% | Validation Acc.: 63.10%\n",
      "Time elapsed: 115.59 min\n",
      "Epoch: 077/100 | Batch 000/192 | Cost: 1.0334\n",
      "Epoch: 077/100 | Batch 120/192 | Cost: 0.9833\n",
      "Epoch: 077/100 Train Acc.: 66.79% | Validation Acc.: 64.40%\n",
      "Time elapsed: 117.40 min\n",
      "Epoch: 078/100 | Batch 000/192 | Cost: 1.0827\n",
      "Epoch: 078/100 | Batch 120/192 | Cost: 0.9324\n",
      "Epoch: 078/100 Train Acc.: 66.47% | Validation Acc.: 62.80%\n",
      "Time elapsed: 119.21 min\n",
      "Epoch: 079/100 | Batch 000/192 | Cost: 0.8738\n",
      "Epoch: 079/100 | Batch 120/192 | Cost: 0.8232\n",
      "Epoch: 079/100 Train Acc.: 66.77% | Validation Acc.: 63.30%\n",
      "Time elapsed: 121.02 min\n",
      "Epoch: 080/100 | Batch 000/192 | Cost: 0.9393\n",
      "Epoch: 080/100 | Batch 120/192 | Cost: 0.9375\n",
      "Epoch: 080/100 Train Acc.: 64.32% | Validation Acc.: 62.10%\n",
      "Time elapsed: 122.84 min\n",
      "Epoch: 081/100 | Batch 000/192 | Cost: 0.9904\n",
      "Epoch: 081/100 | Batch 120/192 | Cost: 0.8718\n",
      "Epoch: 081/100 Train Acc.: 66.10% | Validation Acc.: 62.50%\n",
      "Time elapsed: 124.65 min\n",
      "Epoch: 082/100 | Batch 000/192 | Cost: 1.0005\n",
      "Epoch: 082/100 | Batch 120/192 | Cost: 0.9158\n",
      "Epoch: 082/100 Train Acc.: 66.59% | Validation Acc.: 62.30%\n",
      "Time elapsed: 126.46 min\n",
      "Epoch: 083/100 | Batch 000/192 | Cost: 1.0153\n",
      "Epoch: 083/100 | Batch 120/192 | Cost: 0.9431\n",
      "Epoch: 083/100 Train Acc.: 66.30% | Validation Acc.: 62.60%\n",
      "Time elapsed: 128.28 min\n",
      "Epoch: 084/100 | Batch 000/192 | Cost: 1.0681\n",
      "Epoch: 084/100 | Batch 120/192 | Cost: 0.9828\n",
      "Epoch: 084/100 Train Acc.: 66.89% | Validation Acc.: 65.40%\n",
      "Time elapsed: 130.08 min\n",
      "Epoch: 085/100 | Batch 000/192 | Cost: 0.9975\n",
      "Epoch: 085/100 | Batch 120/192 | Cost: 1.0390\n",
      "Epoch: 085/100 Train Acc.: 67.81% | Validation Acc.: 64.20%\n",
      "Time elapsed: 131.90 min\n",
      "Epoch: 086/100 | Batch 000/192 | Cost: 0.9341\n",
      "Epoch: 086/100 | Batch 120/192 | Cost: 0.8730\n",
      "Epoch: 086/100 Train Acc.: 67.55% | Validation Acc.: 65.20%\n",
      "Time elapsed: 133.71 min\n",
      "Epoch: 087/100 | Batch 000/192 | Cost: 0.9279\n",
      "Epoch: 087/100 | Batch 120/192 | Cost: 0.9615\n",
      "Epoch: 087/100 Train Acc.: 67.95% | Validation Acc.: 65.30%\n",
      "Time elapsed: 135.53 min\n",
      "Epoch: 088/100 | Batch 000/192 | Cost: 0.8909\n",
      "Epoch: 088/100 | Batch 120/192 | Cost: 0.9524\n",
      "Epoch: 088/100 Train Acc.: 65.79% | Validation Acc.: 64.30%\n",
      "Time elapsed: 137.34 min\n",
      "Epoch: 089/100 | Batch 000/192 | Cost: 0.9263\n",
      "Epoch: 089/100 | Batch 120/192 | Cost: 0.9762\n",
      "Epoch: 089/100 Train Acc.: 67.50% | Validation Acc.: 63.20%\n",
      "Time elapsed: 139.15 min\n",
      "Epoch: 090/100 | Batch 000/192 | Cost: 0.8547\n",
      "Epoch: 090/100 | Batch 120/192 | Cost: 0.8747\n",
      "Epoch: 090/100 Train Acc.: 67.69% | Validation Acc.: 65.40%\n",
      "Time elapsed: 140.96 min\n",
      "Epoch: 091/100 | Batch 000/192 | Cost: 0.8149\n",
      "Epoch: 091/100 | Batch 120/192 | Cost: 0.9728\n",
      "Epoch: 091/100 Train Acc.: 66.46% | Validation Acc.: 63.40%\n",
      "Time elapsed: 142.78 min\n",
      "Epoch: 092/100 | Batch 000/192 | Cost: 1.0262\n",
      "Epoch: 092/100 | Batch 120/192 | Cost: 0.9003\n",
      "Epoch: 092/100 Train Acc.: 68.24% | Validation Acc.: 66.40%\n",
      "Time elapsed: 144.59 min\n",
      "Epoch: 093/100 | Batch 000/192 | Cost: 0.8005\n",
      "Epoch: 093/100 | Batch 120/192 | Cost: 0.8742\n",
      "Epoch: 093/100 Train Acc.: 68.38% | Validation Acc.: 65.60%\n",
      "Time elapsed: 146.40 min\n",
      "Epoch: 094/100 | Batch 000/192 | Cost: 0.9519\n",
      "Epoch: 094/100 | Batch 120/192 | Cost: 0.9033\n",
      "Epoch: 094/100 Train Acc.: 67.75% | Validation Acc.: 64.10%\n",
      "Time elapsed: 148.21 min\n",
      "Epoch: 095/100 | Batch 000/192 | Cost: 0.9804\n",
      "Epoch: 095/100 | Batch 120/192 | Cost: 0.8961\n",
      "Epoch: 095/100 Train Acc.: 68.49% | Validation Acc.: 66.30%\n",
      "Time elapsed: 150.03 min\n",
      "Epoch: 096/100 | Batch 000/192 | Cost: 0.7868\n",
      "Epoch: 096/100 | Batch 120/192 | Cost: 0.7790\n",
      "Epoch: 096/100 Train Acc.: 67.39% | Validation Acc.: 65.30%\n",
      "Time elapsed: 151.84 min\n",
      "Epoch: 097/100 | Batch 000/192 | Cost: 0.9255\n",
      "Epoch: 097/100 | Batch 120/192 | Cost: 0.9254\n",
      "Epoch: 097/100 Train Acc.: 68.47% | Validation Acc.: 63.50%\n",
      "Time elapsed: 153.65 min\n",
      "Epoch: 098/100 | Batch 000/192 | Cost: 0.9536\n",
      "Epoch: 098/100 | Batch 120/192 | Cost: 1.0490\n",
      "Epoch: 098/100 Train Acc.: 68.60% | Validation Acc.: 66.00%\n",
      "Time elapsed: 155.47 min\n",
      "Epoch: 099/100 | Batch 000/192 | Cost: 0.9154\n",
      "Epoch: 099/100 | Batch 120/192 | Cost: 0.8630\n",
      "Epoch: 099/100 Train Acc.: 69.60% | Validation Acc.: 67.60%\n",
      "Time elapsed: 157.28 min\n",
      "Epoch: 100/100 | Batch 000/192 | Cost: 0.9177\n",
      "Epoch: 100/100 | Batch 120/192 | Cost: 0.9322\n",
      "Epoch: 100/100 Train Acc.: 66.52% | Validation Acc.: 65.00%\n",
      "Time elapsed: 159.09 min\n",
      "Total Training Time: 159.09 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)  # 将特征数据移到指定设备\n",
    "        targets = targets.to(device)    # 将目标数据移到指定设备\n",
    "\n",
    "        logits, probas = model(features)  # 获取模型输出的logits和概率\n",
    "        _, predicted_labels = torch.max(probas, 1)  # 取最大概率的类别作为预测标签\n",
    "        num_examples += targets.size(0)  # 累加样本数量\n",
    "        correct_pred += (predicted_labels == targets).sum()  # 累加预测正确的样本数\n",
    "    return correct_pred.float()/num_examples * 100  # 返回正确预测率百分比\n",
    "    \n",
    "\n",
    "start_time = time.time()  # 记录训练开始时间\n",
    "for epoch in range(NUM_EPOCHS):  # 遍历每个epoch\n",
    "    \n",
    "    model.train()  # 设置模型为训练模式\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):  # 遍历每个batch\n",
    "    \n",
    "        ### 准备每个小批量数据\n",
    "        features = features.to(DEVICE)  # 将特征数据移到指定设备\n",
    "        targets = targets.to(DEVICE)    # 将目标数据移到指定设备\n",
    "            \n",
    "        ### 前向传播和反向传播\n",
    "        logits, probas = model(features)  # 获取模型输出的logits和概率\n",
    "        cost = F.cross_entropy(logits, targets)  # 计算交叉熵损失\n",
    "        optimizer.zero_grad()  # 清除旧的梯度\n",
    "        \n",
    "        cost.backward()  # 计算梯度\n",
    "        \n",
    "        ### 更新模型参数\n",
    "        optimizer.step()  # 根据梯度更新模型参数\n",
    "        \n",
    "        ### 日志输出\n",
    "        if not batch_idx % 120:  # 每隔120个batch打印一次日志\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')  # 输出当前epoch和batch的损失值\n",
    "\n",
    "    # 在计算准确率时不需要构建计算图以节省内存\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc = compute_accuracy(model, train_loader, device=DEVICE)  # 计算训练集的准确率\n",
    "        valid_acc = compute_accuracy(model, valid_loader, device=DEVICE)  # 计算验证集的准确率\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Validation Acc.: {valid_acc:.2f}%')  # 输出训练集和验证集的准确率\n",
    "        \n",
    "    elapsed = (time.time() - start_time)/60  # 计算训练已经进行的时间\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')  # 输出经过的时间\n",
    "  \n",
    "elapsed = (time.time() - start_time)/60  # 计算总的训练时间\n",
    "print(f'Total Training Time: {elapsed:.2f} min')  # 输出总的训练时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 63.69%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # 在推理过程中禁用梯度计算，节省内存\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### 模型\n",
    "##########################\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NiN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = nn.Sequential(\n",
    "                # 第一层卷积层，输入3通道，输出192通道，卷积核大小为5，步幅为1，padding为2，且没有偏置\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "                # 对192通道进行BatchNorm，标准化处理\n",
    "                nn.BatchNorm2d(192),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                # 第二层卷积层，输入192通道，输出160通道，卷积核大小为1，步幅为1，padding为0，且没有偏置\n",
    "                nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                # 对160通道进行BatchNorm\n",
    "                nn.BatchNorm2d(160),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                # 第三层卷积层，输入160通道，输出96通道，卷积核大小为1，步幅为1，padding为0，且没有偏置\n",
    "                nn.Conv2d(160, 96, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                # 对96通道进行BatchNorm\n",
    "                nn.BatchNorm2d(96),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                # 最大池化层，池化核大小为3，步幅为2，padding为1\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                # Dropout层，丢弃概率为0.5\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                # 第四层卷积层，输入96通道，输出192通道，卷积核大小为5，步幅为1，padding为2，且没有偏置\n",
    "                nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "                # 对192通道进行BatchNorm\n",
    "                nn.BatchNorm2d(192),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                # 第五层卷积层，输入192通道，输出192通道，卷积核大小为1，步幅为1，padding为0，且没有偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                # 对192通道进行BatchNorm\n",
    "                nn.BatchNorm2d(192),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                # 第六层卷积层，输入192通道，输出192通道，卷积核大小为1，步幅为1，padding为0，且没有偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                # 对192通道进行BatchNorm\n",
    "                nn.BatchNorm2d(192),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                # 平均池化层，池化核大小为3，步幅为2，padding为1\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                # Dropout层，丢弃概率为0.5\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                # 第七层卷积层，输入192通道，输出192通道，卷积核大小为3，步幅为1，padding为1，且没有偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                # 对192通道进行BatchNorm\n",
    "                nn.BatchNorm2d(192),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                # 第八层卷积层，输入192通道，输出192通道，卷积核大小为1，步幅为1，padding为0，且没有偏置\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                # 对192通道进行BatchNorm\n",
    "                nn.BatchNorm2d(192),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                # 第九层卷积层，输入192通道，输出10通道，卷积核大小为1，步幅为1，padding为0\n",
    "                nn.Conv2d(192, 10, kernel_size=1, stride=1, padding=0),\n",
    "                # ReLU激活函数\n",
    "                nn.ReLU(inplace=True),\n",
    "                # 平均池化层，池化核大小为8，步幅为1，padding为0\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播，通过分类器进行处理\n",
    "        x = self.classifier(x)\n",
    "        # 将特征图展平成一维向量，作为网络输出的logits\n",
    "        logits = x.view(x.size(0), self.num_classes)\n",
    "        # 使用softmax函数计算类概率分布\n",
    "        probas = torch.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/192 | Cost: 0.9971\n",
      "Epoch: 001/100 | Batch 120/192 | Cost: 0.8200\n",
      "Epoch: 001/100 Train Acc.: 69.53% | Validation Acc.: 65.40%\n",
      "Time elapsed: 1.81 min\n",
      "Epoch: 002/100 | Batch 000/192 | Cost: 0.9075\n",
      "Epoch: 002/100 | Batch 120/192 | Cost: 0.8703\n",
      "Epoch: 002/100 Train Acc.: 67.59% | Validation Acc.: 64.80%\n",
      "Time elapsed: 3.63 min\n",
      "Epoch: 003/100 | Batch 000/192 | Cost: 0.9057\n",
      "Epoch: 003/100 | Batch 120/192 | Cost: 0.9471\n",
      "Epoch: 003/100 Train Acc.: 67.44% | Validation Acc.: 65.10%\n",
      "Time elapsed: 5.44 min\n",
      "Epoch: 004/100 | Batch 000/192 | Cost: 0.9677\n",
      "Epoch: 004/100 | Batch 120/192 | Cost: 0.8854\n",
      "Epoch: 004/100 Train Acc.: 69.69% | Validation Acc.: 66.70%\n",
      "Time elapsed: 7.25 min\n",
      "Epoch: 005/100 | Batch 000/192 | Cost: 0.9167\n",
      "Epoch: 005/100 | Batch 120/192 | Cost: 0.8702\n",
      "Epoch: 005/100 Train Acc.: 67.90% | Validation Acc.: 66.20%\n",
      "Time elapsed: 9.07 min\n",
      "Epoch: 006/100 | Batch 000/192 | Cost: 0.9741\n",
      "Epoch: 006/100 | Batch 120/192 | Cost: 1.0208\n",
      "Epoch: 006/100 Train Acc.: 70.09% | Validation Acc.: 67.00%\n",
      "Time elapsed: 10.88 min\n",
      "Epoch: 007/100 | Batch 000/192 | Cost: 0.8320\n",
      "Epoch: 007/100 | Batch 120/192 | Cost: 0.8854\n",
      "Epoch: 007/100 Train Acc.: 70.33% | Validation Acc.: 67.70%\n",
      "Time elapsed: 12.69 min\n",
      "Epoch: 008/100 | Batch 000/192 | Cost: 0.8304\n",
      "Epoch: 008/100 | Batch 120/192 | Cost: 0.8163\n",
      "Epoch: 008/100 Train Acc.: 70.02% | Validation Acc.: 66.60%\n",
      "Time elapsed: 14.50 min\n",
      "Epoch: 009/100 | Batch 000/192 | Cost: 0.8844\n",
      "Epoch: 009/100 | Batch 120/192 | Cost: 0.8533\n",
      "Epoch: 009/100 Train Acc.: 69.83% | Validation Acc.: 66.30%\n",
      "Time elapsed: 16.31 min\n",
      "Epoch: 010/100 | Batch 000/192 | Cost: 0.7891\n",
      "Epoch: 010/100 | Batch 120/192 | Cost: 0.8235\n",
      "Epoch: 010/100 Train Acc.: 69.70% | Validation Acc.: 66.80%\n",
      "Time elapsed: 18.12 min\n",
      "Epoch: 011/100 | Batch 000/192 | Cost: 0.9432\n",
      "Epoch: 011/100 | Batch 120/192 | Cost: 0.8857\n",
      "Epoch: 011/100 Train Acc.: 69.54% | Validation Acc.: 67.10%\n",
      "Time elapsed: 19.93 min\n",
      "Epoch: 012/100 | Batch 000/192 | Cost: 0.8423\n",
      "Epoch: 012/100 | Batch 120/192 | Cost: 0.8604\n",
      "Epoch: 012/100 Train Acc.: 69.60% | Validation Acc.: 67.40%\n",
      "Time elapsed: 21.74 min\n",
      "Epoch: 013/100 | Batch 000/192 | Cost: 0.8144\n",
      "Epoch: 013/100 | Batch 120/192 | Cost: 0.9436\n",
      "Epoch: 013/100 Train Acc.: 70.99% | Validation Acc.: 66.70%\n",
      "Time elapsed: 23.56 min\n",
      "Epoch: 014/100 | Batch 000/192 | Cost: 0.8200\n",
      "Epoch: 014/100 | Batch 120/192 | Cost: 0.8272\n",
      "Epoch: 014/100 Train Acc.: 69.48% | Validation Acc.: 65.80%\n",
      "Time elapsed: 25.36 min\n",
      "Epoch: 015/100 | Batch 000/192 | Cost: 0.7813\n",
      "Epoch: 015/100 | Batch 120/192 | Cost: 0.8248\n",
      "Epoch: 015/100 Train Acc.: 70.46% | Validation Acc.: 67.70%\n",
      "Time elapsed: 27.18 min\n",
      "Epoch: 016/100 | Batch 000/192 | Cost: 0.8013\n",
      "Epoch: 016/100 | Batch 120/192 | Cost: 0.7891\n",
      "Epoch: 016/100 Train Acc.: 71.20% | Validation Acc.: 66.80%\n",
      "Time elapsed: 28.99 min\n",
      "Epoch: 017/100 | Batch 000/192 | Cost: 0.7615\n",
      "Epoch: 017/100 | Batch 120/192 | Cost: 0.9892\n",
      "Epoch: 017/100 Train Acc.: 68.99% | Validation Acc.: 65.20%\n",
      "Time elapsed: 30.80 min\n",
      "Epoch: 018/100 | Batch 000/192 | Cost: 0.8264\n",
      "Epoch: 018/100 | Batch 120/192 | Cost: 0.9482\n",
      "Epoch: 018/100 Train Acc.: 68.70% | Validation Acc.: 65.20%\n",
      "Time elapsed: 32.61 min\n",
      "Epoch: 019/100 | Batch 000/192 | Cost: 1.0117\n",
      "Epoch: 019/100 | Batch 120/192 | Cost: 0.7937\n",
      "Epoch: 019/100 Train Acc.: 69.30% | Validation Acc.: 66.30%\n",
      "Time elapsed: 34.42 min\n",
      "Epoch: 020/100 | Batch 000/192 | Cost: 0.9864\n",
      "Epoch: 020/100 | Batch 120/192 | Cost: 0.8549\n",
      "Epoch: 020/100 Train Acc.: 70.89% | Validation Acc.: 67.20%\n",
      "Time elapsed: 36.23 min\n",
      "Epoch: 021/100 | Batch 000/192 | Cost: 0.7541\n",
      "Epoch: 021/100 | Batch 120/192 | Cost: 0.7170\n",
      "Epoch: 021/100 Train Acc.: 70.97% | Validation Acc.: 67.10%\n",
      "Time elapsed: 38.05 min\n",
      "Epoch: 022/100 | Batch 000/192 | Cost: 0.7970\n",
      "Epoch: 022/100 | Batch 120/192 | Cost: 0.8517\n",
      "Epoch: 022/100 Train Acc.: 70.71% | Validation Acc.: 66.40%\n",
      "Time elapsed: 39.86 min\n",
      "Epoch: 023/100 | Batch 000/192 | Cost: 0.8842\n",
      "Epoch: 023/100 | Batch 120/192 | Cost: 0.8351\n",
      "Epoch: 023/100 Train Acc.: 71.22% | Validation Acc.: 68.10%\n",
      "Time elapsed: 41.67 min\n",
      "Epoch: 024/100 | Batch 000/192 | Cost: 0.7131\n",
      "Epoch: 024/100 | Batch 120/192 | Cost: 0.8367\n",
      "Epoch: 024/100 Train Acc.: 71.49% | Validation Acc.: 69.10%\n",
      "Time elapsed: 43.48 min\n",
      "Epoch: 025/100 | Batch 000/192 | Cost: 0.7318\n",
      "Epoch: 025/100 | Batch 120/192 | Cost: 0.7792\n",
      "Epoch: 025/100 Train Acc.: 71.04% | Validation Acc.: 66.50%\n",
      "Time elapsed: 45.29 min\n",
      "Epoch: 026/100 | Batch 000/192 | Cost: 0.8421\n",
      "Epoch: 026/100 | Batch 120/192 | Cost: 0.8602\n",
      "Epoch: 026/100 Train Acc.: 70.55% | Validation Acc.: 68.70%\n",
      "Time elapsed: 47.10 min\n",
      "Epoch: 027/100 | Batch 000/192 | Cost: 0.7894\n",
      "Epoch: 027/100 | Batch 120/192 | Cost: 0.8182\n",
      "Epoch: 027/100 Train Acc.: 71.82% | Validation Acc.: 67.50%\n",
      "Time elapsed: 48.91 min\n",
      "Epoch: 028/100 | Batch 000/192 | Cost: 0.8641\n",
      "Epoch: 028/100 | Batch 120/192 | Cost: 1.0308\n",
      "Epoch: 028/100 Train Acc.: 69.97% | Validation Acc.: 66.60%\n",
      "Time elapsed: 50.73 min\n",
      "Epoch: 029/100 | Batch 000/192 | Cost: 0.8930\n",
      "Epoch: 029/100 | Batch 120/192 | Cost: 0.8810\n",
      "Epoch: 029/100 Train Acc.: 71.71% | Validation Acc.: 69.80%\n",
      "Time elapsed: 52.54 min\n",
      "Epoch: 030/100 | Batch 000/192 | Cost: 0.8623\n",
      "Epoch: 030/100 | Batch 120/192 | Cost: 0.9480\n",
      "Epoch: 030/100 Train Acc.: 70.08% | Validation Acc.: 66.70%\n",
      "Time elapsed: 54.35 min\n",
      "Epoch: 031/100 | Batch 000/192 | Cost: 0.8324\n",
      "Epoch: 031/100 | Batch 120/192 | Cost: 0.7578\n",
      "Epoch: 031/100 Train Acc.: 70.86% | Validation Acc.: 67.80%\n",
      "Time elapsed: 56.16 min\n",
      "Epoch: 032/100 | Batch 000/192 | Cost: 0.7638\n",
      "Epoch: 032/100 | Batch 120/192 | Cost: 0.7658\n",
      "Epoch: 032/100 Train Acc.: 70.57% | Validation Acc.: 64.90%\n",
      "Time elapsed: 57.97 min\n",
      "Epoch: 033/100 | Batch 000/192 | Cost: 0.8845\n",
      "Epoch: 033/100 | Batch 120/192 | Cost: 0.7205\n",
      "Epoch: 033/100 Train Acc.: 71.98% | Validation Acc.: 67.80%\n",
      "Time elapsed: 59.78 min\n",
      "Epoch: 034/100 | Batch 000/192 | Cost: 0.7432\n",
      "Epoch: 034/100 | Batch 120/192 | Cost: 0.7372\n",
      "Epoch: 034/100 Train Acc.: 72.09% | Validation Acc.: 69.10%\n",
      "Time elapsed: 61.59 min\n",
      "Epoch: 035/100 | Batch 000/192 | Cost: 0.8486\n",
      "Epoch: 035/100 | Batch 120/192 | Cost: 0.7847\n",
      "Epoch: 035/100 Train Acc.: 72.02% | Validation Acc.: 67.80%\n",
      "Time elapsed: 63.40 min\n",
      "Epoch: 036/100 | Batch 000/192 | Cost: 0.7711\n",
      "Epoch: 036/100 | Batch 120/192 | Cost: 0.7973\n",
      "Epoch: 036/100 Train Acc.: 71.86% | Validation Acc.: 69.00%\n",
      "Time elapsed: 65.21 min\n",
      "Epoch: 037/100 | Batch 000/192 | Cost: 0.8906\n",
      "Epoch: 037/100 | Batch 120/192 | Cost: 0.7359\n",
      "Epoch: 037/100 Train Acc.: 72.84% | Validation Acc.: 69.60%\n",
      "Time elapsed: 67.03 min\n",
      "Epoch: 038/100 | Batch 000/192 | Cost: 0.7896\n",
      "Epoch: 038/100 | Batch 120/192 | Cost: 0.8501\n",
      "Epoch: 038/100 Train Acc.: 71.06% | Validation Acc.: 69.40%\n",
      "Time elapsed: 68.84 min\n",
      "Epoch: 039/100 | Batch 000/192 | Cost: 0.8313\n",
      "Epoch: 039/100 | Batch 120/192 | Cost: 0.9081\n",
      "Epoch: 039/100 Train Acc.: 78.52% | Validation Acc.: 74.50%\n",
      "Time elapsed: 70.65 min\n",
      "Epoch: 040/100 | Batch 000/192 | Cost: 0.7304\n",
      "Epoch: 040/100 | Batch 120/192 | Cost: 0.5977\n",
      "Epoch: 040/100 Train Acc.: 78.42% | Validation Acc.: 74.20%\n",
      "Time elapsed: 72.46 min\n",
      "Epoch: 041/100 | Batch 000/192 | Cost: 0.6897\n",
      "Epoch: 041/100 | Batch 120/192 | Cost: 0.6751\n",
      "Epoch: 041/100 Train Acc.: 79.48% | Validation Acc.: 77.20%\n",
      "Time elapsed: 74.27 min\n",
      "Epoch: 042/100 | Batch 000/192 | Cost: 0.5430\n",
      "Epoch: 042/100 | Batch 120/192 | Cost: 0.4868\n",
      "Epoch: 042/100 Train Acc.: 79.74% | Validation Acc.: 77.70%\n",
      "Time elapsed: 76.08 min\n",
      "Epoch: 043/100 | Batch 000/192 | Cost: 0.6105\n",
      "Epoch: 043/100 | Batch 120/192 | Cost: 0.6387\n",
      "Epoch: 043/100 Train Acc.: 80.48% | Validation Acc.: 77.70%\n",
      "Time elapsed: 77.89 min\n",
      "Epoch: 044/100 | Batch 000/192 | Cost: 0.6365\n",
      "Epoch: 044/100 | Batch 120/192 | Cost: 0.5360\n",
      "Epoch: 044/100 Train Acc.: 79.76% | Validation Acc.: 76.10%\n",
      "Time elapsed: 79.70 min\n",
      "Epoch: 045/100 | Batch 000/192 | Cost: 0.6262\n",
      "Epoch: 045/100 | Batch 120/192 | Cost: 0.5460\n",
      "Epoch: 045/100 Train Acc.: 79.37% | Validation Acc.: 75.00%\n",
      "Time elapsed: 81.51 min\n",
      "Epoch: 046/100 | Batch 000/192 | Cost: 0.5754\n",
      "Epoch: 046/100 | Batch 120/192 | Cost: 0.5584\n",
      "Epoch: 046/100 Train Acc.: 77.73% | Validation Acc.: 75.00%\n",
      "Time elapsed: 83.32 min\n",
      "Epoch: 047/100 | Batch 000/192 | Cost: 0.7102\n",
      "Epoch: 047/100 | Batch 120/192 | Cost: 0.6338\n",
      "Epoch: 047/100 Train Acc.: 80.85% | Validation Acc.: 77.60%\n",
      "Time elapsed: 85.14 min\n",
      "Epoch: 048/100 | Batch 000/192 | Cost: 0.5964\n",
      "Epoch: 048/100 | Batch 120/192 | Cost: 0.6120\n",
      "Epoch: 048/100 Train Acc.: 79.88% | Validation Acc.: 77.60%\n",
      "Time elapsed: 86.95 min\n",
      "Epoch: 049/100 | Batch 000/192 | Cost: 0.6153\n",
      "Epoch: 049/100 | Batch 120/192 | Cost: 0.5796\n",
      "Epoch: 049/100 Train Acc.: 81.00% | Validation Acc.: 75.00%\n",
      "Time elapsed: 88.76 min\n",
      "Epoch: 050/100 | Batch 000/192 | Cost: 0.5786\n",
      "Epoch: 050/100 | Batch 120/192 | Cost: 0.5151\n",
      "Epoch: 050/100 Train Acc.: 80.55% | Validation Acc.: 77.30%\n",
      "Time elapsed: 90.57 min\n",
      "Epoch: 051/100 | Batch 000/192 | Cost: 0.4829\n",
      "Epoch: 051/100 | Batch 120/192 | Cost: 0.4396\n",
      "Epoch: 051/100 Train Acc.: 80.57% | Validation Acc.: 76.00%\n",
      "Time elapsed: 92.38 min\n",
      "Epoch: 052/100 | Batch 000/192 | Cost: 0.5660\n",
      "Epoch: 052/100 | Batch 120/192 | Cost: 0.6306\n",
      "Epoch: 052/100 Train Acc.: 79.59% | Validation Acc.: 76.10%\n",
      "Time elapsed: 94.03 min\n",
      "Epoch: 053/100 | Batch 000/192 | Cost: 0.5662\n",
      "Epoch: 053/100 | Batch 120/192 | Cost: 0.5520\n",
      "Epoch: 053/100 Train Acc.: 81.52% | Validation Acc.: 77.70%\n",
      "Time elapsed: 94.75 min\n",
      "Epoch: 054/100 | Batch 000/192 | Cost: 0.5300\n",
      "Epoch: 054/100 | Batch 120/192 | Cost: 0.5458\n",
      "Epoch: 054/100 Train Acc.: 79.92% | Validation Acc.: 76.00%\n",
      "Time elapsed: 95.47 min\n",
      "Epoch: 055/100 | Batch 000/192 | Cost: 0.5279\n",
      "Epoch: 055/100 | Batch 120/192 | Cost: 0.4915\n",
      "Epoch: 055/100 Train Acc.: 81.20% | Validation Acc.: 77.60%\n",
      "Time elapsed: 96.19 min\n",
      "Epoch: 056/100 | Batch 000/192 | Cost: 0.5881\n",
      "Epoch: 056/100 | Batch 120/192 | Cost: 0.5566\n",
      "Epoch: 056/100 Train Acc.: 80.66% | Validation Acc.: 76.50%\n",
      "Time elapsed: 96.92 min\n",
      "Epoch: 057/100 | Batch 000/192 | Cost: 0.4934\n",
      "Epoch: 057/100 | Batch 120/192 | Cost: 0.5914\n",
      "Epoch: 057/100 Train Acc.: 82.04% | Validation Acc.: 78.90%\n",
      "Time elapsed: 97.64 min\n",
      "Epoch: 058/100 | Batch 000/192 | Cost: 0.4922\n",
      "Epoch: 058/100 | Batch 120/192 | Cost: 0.6397\n",
      "Epoch: 058/100 Train Acc.: 81.54% | Validation Acc.: 77.70%\n",
      "Time elapsed: 98.36 min\n",
      "Epoch: 059/100 | Batch 000/192 | Cost: 0.4897\n",
      "Epoch: 059/100 | Batch 120/192 | Cost: 0.5482\n",
      "Epoch: 059/100 Train Acc.: 82.03% | Validation Acc.: 76.60%\n",
      "Time elapsed: 99.08 min\n",
      "Epoch: 060/100 | Batch 000/192 | Cost: 0.5467\n",
      "Epoch: 060/100 | Batch 120/192 | Cost: 0.4420\n",
      "Epoch: 060/100 Train Acc.: 81.46% | Validation Acc.: 78.00%\n",
      "Time elapsed: 99.80 min\n",
      "Epoch: 061/100 | Batch 000/192 | Cost: 0.4882\n",
      "Epoch: 061/100 | Batch 120/192 | Cost: 0.7006\n",
      "Epoch: 061/100 Train Acc.: 82.25% | Validation Acc.: 79.60%\n",
      "Time elapsed: 100.52 min\n",
      "Epoch: 062/100 | Batch 000/192 | Cost: 0.4362\n",
      "Epoch: 062/100 | Batch 120/192 | Cost: 0.5278\n",
      "Epoch: 062/100 Train Acc.: 82.76% | Validation Acc.: 78.10%\n",
      "Time elapsed: 101.24 min\n",
      "Epoch: 063/100 | Batch 000/192 | Cost: 0.4742\n",
      "Epoch: 063/100 | Batch 120/192 | Cost: 0.5125\n",
      "Epoch: 063/100 Train Acc.: 81.13% | Validation Acc.: 76.40%\n",
      "Time elapsed: 101.96 min\n",
      "Epoch: 064/100 | Batch 000/192 | Cost: 0.5822\n",
      "Epoch: 064/100 | Batch 120/192 | Cost: 0.5916\n",
      "Epoch: 064/100 Train Acc.: 81.53% | Validation Acc.: 75.90%\n",
      "Time elapsed: 102.68 min\n",
      "Epoch: 065/100 | Batch 000/192 | Cost: 0.4885\n",
      "Epoch: 065/100 | Batch 120/192 | Cost: 0.6692\n",
      "Epoch: 065/100 Train Acc.: 80.56% | Validation Acc.: 76.70%\n",
      "Time elapsed: 103.40 min\n",
      "Epoch: 066/100 | Batch 000/192 | Cost: 0.6398\n",
      "Epoch: 066/100 | Batch 120/192 | Cost: 0.5624\n",
      "Epoch: 066/100 Train Acc.: 81.41% | Validation Acc.: 77.00%\n",
      "Time elapsed: 104.12 min\n",
      "Epoch: 067/100 | Batch 000/192 | Cost: 0.6211\n",
      "Epoch: 067/100 | Batch 120/192 | Cost: 0.5655\n",
      "Epoch: 067/100 Train Acc.: 82.20% | Validation Acc.: 75.80%\n",
      "Time elapsed: 104.84 min\n",
      "Epoch: 068/100 | Batch 000/192 | Cost: 0.4977\n",
      "Epoch: 068/100 | Batch 120/192 | Cost: 0.5702\n",
      "Epoch: 068/100 Train Acc.: 82.15% | Validation Acc.: 78.20%\n",
      "Time elapsed: 105.56 min\n",
      "Epoch: 069/100 | Batch 000/192 | Cost: 0.6046\n",
      "Epoch: 069/100 | Batch 120/192 | Cost: 0.4847\n",
      "Epoch: 069/100 Train Acc.: 81.08% | Validation Acc.: 76.70%\n",
      "Time elapsed: 106.29 min\n",
      "Epoch: 070/100 | Batch 000/192 | Cost: 0.5546\n",
      "Epoch: 070/100 | Batch 120/192 | Cost: 0.4977\n",
      "Epoch: 070/100 Train Acc.: 81.95% | Validation Acc.: 76.70%\n",
      "Time elapsed: 107.01 min\n",
      "Epoch: 071/100 | Batch 000/192 | Cost: 0.5588\n",
      "Epoch: 071/100 | Batch 120/192 | Cost: 0.5458\n",
      "Epoch: 071/100 Train Acc.: 80.67% | Validation Acc.: 77.50%\n",
      "Time elapsed: 107.73 min\n",
      "Epoch: 072/100 | Batch 000/192 | Cost: 0.5350\n",
      "Epoch: 072/100 | Batch 120/192 | Cost: 0.4624\n",
      "Epoch: 072/100 Train Acc.: 81.47% | Validation Acc.: 78.30%\n",
      "Time elapsed: 108.45 min\n",
      "Epoch: 073/100 | Batch 000/192 | Cost: 0.5526\n",
      "Epoch: 073/100 | Batch 120/192 | Cost: 0.5100\n",
      "Epoch: 073/100 Train Acc.: 82.58% | Validation Acc.: 76.90%\n",
      "Time elapsed: 109.17 min\n",
      "Epoch: 074/100 | Batch 000/192 | Cost: 0.5860\n",
      "Epoch: 074/100 | Batch 120/192 | Cost: 0.5225\n",
      "Epoch: 074/100 Train Acc.: 82.29% | Validation Acc.: 78.70%\n",
      "Time elapsed: 109.89 min\n",
      "Epoch: 075/100 | Batch 000/192 | Cost: 0.4707\n",
      "Epoch: 075/100 | Batch 120/192 | Cost: 0.5407\n",
      "Epoch: 075/100 Train Acc.: 81.82% | Validation Acc.: 78.20%\n",
      "Time elapsed: 110.61 min\n",
      "Epoch: 076/100 | Batch 000/192 | Cost: 0.5226\n",
      "Epoch: 076/100 | Batch 120/192 | Cost: 0.6105\n",
      "Epoch: 076/100 Train Acc.: 82.70% | Validation Acc.: 78.50%\n",
      "Time elapsed: 111.33 min\n",
      "Epoch: 077/100 | Batch 000/192 | Cost: 0.4869\n",
      "Epoch: 077/100 | Batch 120/192 | Cost: 0.4974\n",
      "Epoch: 077/100 Train Acc.: 82.63% | Validation Acc.: 78.50%\n",
      "Time elapsed: 112.05 min\n",
      "Epoch: 078/100 | Batch 000/192 | Cost: 0.5092\n",
      "Epoch: 078/100 | Batch 120/192 | Cost: 0.5152\n",
      "Epoch: 078/100 Train Acc.: 82.43% | Validation Acc.: 77.30%\n",
      "Time elapsed: 112.77 min\n",
      "Epoch: 079/100 | Batch 000/192 | Cost: 0.5222\n",
      "Epoch: 079/100 | Batch 120/192 | Cost: 0.4481\n",
      "Epoch: 079/100 Train Acc.: 83.26% | Validation Acc.: 77.80%\n",
      "Time elapsed: 113.49 min\n",
      "Epoch: 080/100 | Batch 000/192 | Cost: 0.5335\n",
      "Epoch: 080/100 | Batch 120/192 | Cost: 0.5481\n",
      "Epoch: 080/100 Train Acc.: 83.14% | Validation Acc.: 80.20%\n",
      "Time elapsed: 114.21 min\n",
      "Epoch: 081/100 | Batch 000/192 | Cost: 0.5053\n",
      "Epoch: 081/100 | Batch 120/192 | Cost: 0.5002\n",
      "Epoch: 081/100 Train Acc.: 83.45% | Validation Acc.: 79.60%\n",
      "Time elapsed: 114.93 min\n",
      "Epoch: 082/100 | Batch 000/192 | Cost: 0.4089\n",
      "Epoch: 082/100 | Batch 120/192 | Cost: 0.6149\n",
      "Epoch: 082/100 Train Acc.: 82.82% | Validation Acc.: 78.50%\n",
      "Time elapsed: 115.66 min\n",
      "Epoch: 083/100 | Batch 000/192 | Cost: 0.5014\n",
      "Epoch: 083/100 | Batch 120/192 | Cost: 0.4271\n",
      "Epoch: 083/100 Train Acc.: 81.97% | Validation Acc.: 77.60%\n",
      "Time elapsed: 116.38 min\n",
      "Epoch: 084/100 | Batch 000/192 | Cost: 0.5037\n",
      "Epoch: 084/100 | Batch 120/192 | Cost: 0.4904\n",
      "Epoch: 084/100 Train Acc.: 81.75% | Validation Acc.: 76.00%\n",
      "Time elapsed: 117.10 min\n",
      "Epoch: 085/100 | Batch 000/192 | Cost: 0.4889\n",
      "Epoch: 085/100 | Batch 120/192 | Cost: 0.6561\n",
      "Epoch: 085/100 Train Acc.: 83.71% | Validation Acc.: 79.00%\n",
      "Time elapsed: 117.82 min\n",
      "Epoch: 086/100 | Batch 000/192 | Cost: 0.4583\n",
      "Epoch: 086/100 | Batch 120/192 | Cost: 0.4936\n",
      "Epoch: 086/100 Train Acc.: 83.03% | Validation Acc.: 78.10%\n",
      "Time elapsed: 118.54 min\n",
      "Epoch: 087/100 | Batch 000/192 | Cost: 0.5568\n",
      "Epoch: 087/100 | Batch 120/192 | Cost: 0.5895\n",
      "Epoch: 087/100 Train Acc.: 83.85% | Validation Acc.: 79.40%\n",
      "Time elapsed: 119.26 min\n",
      "Epoch: 088/100 | Batch 000/192 | Cost: 0.4255\n",
      "Epoch: 088/100 | Batch 120/192 | Cost: 0.3787\n",
      "Epoch: 088/100 Train Acc.: 80.51% | Validation Acc.: 77.40%\n",
      "Time elapsed: 119.98 min\n",
      "Epoch: 089/100 | Batch 000/192 | Cost: 0.4345\n",
      "Epoch: 089/100 | Batch 120/192 | Cost: 0.4591\n",
      "Epoch: 089/100 Train Acc.: 83.87% | Validation Acc.: 79.50%\n",
      "Time elapsed: 120.70 min\n",
      "Epoch: 090/100 | Batch 000/192 | Cost: 0.4173\n",
      "Epoch: 090/100 | Batch 120/192 | Cost: 0.5918\n",
      "Epoch: 090/100 Train Acc.: 83.03% | Validation Acc.: 78.20%\n",
      "Time elapsed: 121.42 min\n",
      "Epoch: 091/100 | Batch 000/192 | Cost: 0.4599\n",
      "Epoch: 091/100 | Batch 120/192 | Cost: 0.3890\n",
      "Epoch: 091/100 Train Acc.: 84.24% | Validation Acc.: 81.30%\n",
      "Time elapsed: 122.14 min\n",
      "Epoch: 092/100 | Batch 000/192 | Cost: 0.4675\n",
      "Epoch: 092/100 | Batch 120/192 | Cost: 0.6018\n",
      "Epoch: 092/100 Train Acc.: 83.07% | Validation Acc.: 77.90%\n",
      "Time elapsed: 122.86 min\n",
      "Epoch: 093/100 | Batch 000/192 | Cost: 0.4646\n",
      "Epoch: 093/100 | Batch 120/192 | Cost: 0.4701\n",
      "Epoch: 093/100 Train Acc.: 83.87% | Validation Acc.: 77.40%\n",
      "Time elapsed: 123.58 min\n",
      "Epoch: 094/100 | Batch 000/192 | Cost: 0.4685\n",
      "Epoch: 094/100 | Batch 120/192 | Cost: 0.4413\n",
      "Epoch: 094/100 Train Acc.: 84.65% | Validation Acc.: 80.90%\n",
      "Time elapsed: 124.30 min\n",
      "Epoch: 095/100 | Batch 000/192 | Cost: 0.4800\n",
      "Epoch: 095/100 | Batch 120/192 | Cost: 0.5409\n",
      "Epoch: 095/100 Train Acc.: 82.77% | Validation Acc.: 76.90%\n",
      "Time elapsed: 125.02 min\n",
      "Epoch: 096/100 | Batch 000/192 | Cost: 0.4057\n",
      "Epoch: 096/100 | Batch 120/192 | Cost: 0.4783\n",
      "Epoch: 096/100 Train Acc.: 84.15% | Validation Acc.: 79.10%\n",
      "Time elapsed: 125.75 min\n",
      "Epoch: 097/100 | Batch 000/192 | Cost: 0.5205\n",
      "Epoch: 097/100 | Batch 120/192 | Cost: 0.4637\n",
      "Epoch: 097/100 Train Acc.: 84.10% | Validation Acc.: 78.10%\n",
      "Time elapsed: 126.47 min\n",
      "Epoch: 098/100 | Batch 000/192 | Cost: 0.4798\n",
      "Epoch: 098/100 | Batch 120/192 | Cost: 0.4111\n",
      "Epoch: 098/100 Train Acc.: 83.65% | Validation Acc.: 79.40%\n",
      "Time elapsed: 127.19 min\n",
      "Epoch: 099/100 | Batch 000/192 | Cost: 0.5007\n",
      "Epoch: 099/100 | Batch 120/192 | Cost: 0.4407\n",
      "Epoch: 099/100 Train Acc.: 83.19% | Validation Acc.: 78.60%\n",
      "Time elapsed: 127.91 min\n",
      "Epoch: 100/100 | Batch 000/192 | Cost: 0.5779\n",
      "Epoch: 100/100 | Batch 120/192 | Cost: 0.4621\n",
      "Epoch: 100/100 Train Acc.: 83.51% | Validation Acc.: 78.10%\n",
      "Time elapsed: 128.63 min\n",
      "Total Training Time: 128.63 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # 记录开始时间\n",
    "for epoch in range(NUM_EPOCHS):  # 循环遍历每个epoch\n",
    "    \n",
    "    model.train()  # 设置模型为训练模式\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):  # 循环遍历每个批次\n",
    "    \n",
    "        ### 准备小批量数据\n",
    "        features = features.to(DEVICE)  # 将特征转移到指定设备\n",
    "        targets = targets.to(DEVICE)  # 将目标标签转移到指定设备\n",
    "            \n",
    "        ### 前向传播和反向传播\n",
    "        logits, probas = model(features)  # 获取模型的输出\n",
    "        cost = F.cross_entropy(logits, targets)  # 计算交叉熵损失\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        \n",
    "        cost.backward()  # 反向传播计算梯度\n",
    "        \n",
    "        ### 更新模型参数\n",
    "        optimizer.step()  # 使用优化器更新参数\n",
    "        \n",
    "        ### 日志记录\n",
    "        if not batch_idx % 120:  # 每120个批次输出一次日志\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "\n",
    "    # 计算准确率时不需要构建计算图以进行反向传播\n",
    "    with torch.set_grad_enabled(False):  # 禁用梯度计算\n",
    "        train_acc = compute_accuracy(model, train_loader, device=DEVICE)  # 计算训练集准确率\n",
    "        valid_acc = compute_accuracy(model, valid_loader, device=DEVICE)  # 计算验证集准确率\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Validation Acc.: {valid_acc:.2f}%')  # 输出准确率\n",
    "    \n",
    "    elapsed = (time.time() - start_time)/60  # 计算已经过去的时间（分钟）\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')  # 输出已用时间\n",
    "  \n",
    "elapsed = (time.time() - start_time)/60  # 计算总训练时间（分钟）\n",
    "print(f'Total Training Time: {elapsed:.2f} min')  # 输出总训练时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 77.02%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # 在推理过程中禁用梯度计算，节省内存\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
