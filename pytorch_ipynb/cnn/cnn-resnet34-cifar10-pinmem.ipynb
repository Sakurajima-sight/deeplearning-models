{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEBilEjLj5wY"
   },
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1524974472601,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "GOzuY8Yvj5wb",
    "outputId": "c19362ce-f87a-4cc2-84cc-8d7b4b9e6007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Sebastian Raschka\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.11\n",
      "IPython version      : 9.0.2\n",
      "\n",
      "torch: 2.6.0+cu126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4XmErYj5wm"
   },
   "source": [
    "# Model Zoo -- ResNet-34 CIFAR-10 Classifier with Pinned Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook comparing the speed of model training with and without using page-locked memory.  \n",
    "这是一个示例笔记本，比较使用和不使用页锁定内存时模型训练的速度。\n",
    "\n",
    "Page-locked memory can be enabled by setting `pin_memory=True` in PyTorch's `DataLoader` class (disabled by default).  \n",
    "可以通过在PyTorch的`DataLoader`类中设置`pin_memory=True`来启用页锁定内存（默认情况下是禁用的）。\n",
    "\n",
    "Theoretically, pinning the memory should speed up the data transfer rate but minimizing the data transfer cost between CPU and the CUDA device; hence, enabling `pin_memory=True` should make the model training faster by some small margin.  \n",
    "从理论上讲，锁定内存应该加快数据传输速率，并减少CPU和CUDA设备之间的数据传输成本；因此，启用`pin_memory=True`应该会使模型训练速度有所提高。\n",
    "\n",
    "> Host (CPU) data allocations are pageable by default. The GPU cannot access data directly from pageable host memory, so when a data transfer from pageable host memory to device memory is invoked, the CUDA driver must first allocate a temporary page-locked, or “pinned”, host array, copy the host data to the pinned array, and then transfer the data from the pinned array to device memory, as illustrated below... (Source: https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/)  \n",
    "> 主机（CPU）数据分配默认是可分页的。GPU无法直接从可分页的主机内存访问数据，因此当从可分页的主机内存向设备内存传输数据时，CUDA驱动程序必须首先分配一个临时的页锁定或“固定”主机数组，将主机数据复制到固定数组中，然后将数据从固定数组传输到设备内存，如下所示……（来源：https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/）\n",
    "\n",
    "After the Model preamble, this Notebook is divided into two subsections, \"Training Without Pinned Memory\" and \"Training with Pinned Memory\" to investigate whether there is a noticeable training time difference when toggling `pin_memory` on and off.  \n",
    "在模型前言之后，本笔记本分为两个子部分，“不使用页锁定内存的训练”和“使用页锁定内存的训练”，以调查在开启和关闭`pin_memory`时是否存在显著的训练时间差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network in this notebook is an implementation of the ResNet-34 [1] architecture on the MNIST digits dataset (http://yann.lecun.com/exdb/mnist/) to train a handwritten digit classifier.  \n",
    "本笔记本中的网络是基于MNIST数字数据集（http://yann.lecun.com/exdb/mnist/）实现的ResNet-34 [1]架构，用于训练手写数字分类器。\n",
    "\n",
    "References  \n",
    "参考文献\n",
    "\n",
    "- [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). ([CVPR Link](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html))  \n",
    "- [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). 深度残差学习用于图像识别。在IEEE计算机视觉与模式识别会议论文集（第770-778页）。([CVPR 链接](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html))\n",
    "\n",
    "- [2] http://yann.lecun.com/exdb/mnist/  \n",
    "- [2] http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "![](../images/resnets/resnet34/resnet34-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure illustrates residual blocks with skip connections such that the input passed via the shortcut matches the dimensions of the main path's output, which allows the network to learn identity functions.  \n",
    "下图展示了带有跳跃连接的残差块，使得通过快捷方式传递的输入与主路径输出的维度相匹配，从而使网络能够学习恒等函数。\n",
    "\n",
    "![](../images/resnets/resnet-ex-1-1.png)\n",
    "\n",
    "The ResNet-34 architecture actually uses residual blocks with skip connections such that the input passed via the shortcut matches is resized to dimensions of the main path's output. Such a residual block is illustrated below:  \n",
    "ResNet-34架构实际上使用带有跳跃连接的残差块，使得通过快捷方式传递的输入被调整为与主路径输出的维度匹配。下图展示了这样的残差块：\n",
    "\n",
    "![](../images/resnets/resnet-ex-1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed explanation see the other notebook, [resnet-ex-1.ipynb](resnet-ex-1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkoGLH_Tj5wn"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.set_per_process_memory_fraction(0.5, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### 设置\n",
    "##########################\n",
    "\n",
    "# 超参数\n",
    "RANDOM_SEED = 1  # 随机种子，确保实验结果可复现\n",
    "LEARNING_RATE = 0.001  # 学习率\n",
    "BATCH_SIZE = 256  # 批次大小\n",
    "NUM_EPOCHS = 10  # 训练轮数\n",
    "\n",
    "# 网络架构\n",
    "NUM_FEATURES = 28*28  # 输入特征数（假设输入图像大小为28x28）\n",
    "NUM_CLASSES = 10  # 分类数量（例如：手写数字分类任务中的0-9，共10类）\n",
    "\n",
    "# 其他设置\n",
    "DEVICE = \"cuda:0\"  # 指定使用的设备，这里选择第二个GPU（cuda:0）\n",
    "GRAYSCALE = False  # 是否使用灰度图像（False表示使用RGB图像）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell that implements the ResNet-34 architecture is a derivative of the code provided at https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html.  \n",
    "以下实现ResNet-34架构的代码单元是基于https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html提供的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### 模型\n",
    "##########################\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"带填充的3x3卷积\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)  # 使用3x3卷积核，步幅为stride，填充为1，bias=False表示不使用偏置\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # 扩展因子，表示每个基本块的输出通道数与输入通道数的关系\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)  # 第1个卷积层\n",
    "        self.bn1 = nn.BatchNorm2d(planes)  # 批归一化\n",
    "        self.relu = nn.ReLU(inplace=True)  # ReLU激活函数\n",
    "        self.conv2 = conv3x3(planes, planes)  # 第2个卷积层\n",
    "        self.bn2 = nn.BatchNorm2d(planes)  # 批归一化\n",
    "        self.downsample = downsample  # 用于下采样的层\n",
    "        self.stride = stride  # 步幅\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # 保存输入x，用于残差连接\n",
    "\n",
    "        out = self.conv1(x)  # 通过第1个卷积层\n",
    "        out = self.bn1(out)  # 批归一化\n",
    "        out = self.relu(out)  # ReLU激活函数\n",
    "\n",
    "        out = self.conv2(out)  # 通过第2个卷积层\n",
    "        out = self.bn2(out)  # 批归一化\n",
    "\n",
    "        if self.downsample is not None:  # 如果需要下采样，则应用下采样操作\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual  # 残差连接\n",
    "        out = self.relu(out)  # ReLU激活函数\n",
    "\n",
    "        return out  # 返回结果\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64  # 初始输入通道数\n",
    "        if grayscale:\n",
    "            in_dim = 1  # 如果是灰度图，输入通道数为1\n",
    "        else:\n",
    "            in_dim = 3  # 如果是彩色图，输入通道数为3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,  # 第一层卷积\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # 批归一化\n",
    "        self.relu = nn.ReLU(inplace=True)  # ReLU激活函数\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 最大池化层\n",
    "        # 创建四个残差块层\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)  # 平均池化层\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)  # 全连接层，用于分类\n",
    "\n",
    "        # 初始化网络中的参数\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):  # 对卷积层进行初始化\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):  # 对批归一化层进行初始化\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            # 如果步幅不等于1或者输入通道数与输出通道数不匹配，则需要下采样\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))  # 添加第一个残差块\n",
    "        self.inplanes = planes * block.expansion  # 更新输入通道数\n",
    "        for i in range(1, blocks):  # 添加后续的残差块\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)  # 返回由残差块组成的层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # 第1个卷积层\n",
    "        x = self.bn1(x)  # 批归一化\n",
    "        x = self.relu(x)  # ReLU激活函数\n",
    "        x = self.maxpool(x)  # 最大池化层\n",
    "\n",
    "        x = self.layer1(x)  # 第1个残差层\n",
    "        x = self.layer2(x)  # 第2个残差层\n",
    "        x = self.layer3(x)  # 第3个残差层\n",
    "        x = self.layer4(x)  # 第4个残差层\n",
    "\n",
    "        # 由于MNIST图像的尺寸已经是1x1，因此可以跳过平均池化层\n",
    "        # 如果输入图像较大，可以启用avgpool\n",
    "        # x = self.avgpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # 展平张量\n",
    "        logits = self.fc(x)  # 全连接层\n",
    "        probas = F.softmax(logits, dim=1)  # softmax得到概率分布\n",
    "        return logits, probas  # 返回logits和概率分布\n",
    "\n",
    "\n",
    "def resnet34(num_classes):\n",
    "    \"\"\"构建ResNet-34模型\"\"\"\n",
    "    model = ResNet(block=BasicBlock, \n",
    "                   layers=[3, 4, 6, 3],  # ResNet-34的层数配置\n",
    "                   num_classes=num_classes,\n",
    "                   grayscale=GRAYSCALE)  # 是否使用灰度图像\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAodboScj5w6"
   },
   "source": [
    "## Training without Pinned Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像批次维度: torch.Size([256, 3, 32, 32])\n",
      "图像标签维度: torch.Size([256])\n",
      "图像批次维度: torch.Size([256, 3, 32, 32])\n",
      "图像标签维度: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### CIFAR-10 数据集\n",
    "##########################\n",
    "\n",
    "\n",
    "# 注意 transforms.ToTensor() 会将输入图像缩放到 0-1 范围\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True,  # 训练集\n",
    "                                 transform=transforms.ToTensor(),  # 转换为Tensor格式\n",
    "                                 download=True)  # 如果数据集不存在，下载数据集\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False,  # 测试集\n",
    "                                transform=transforms.ToTensor())  # 转换为Tensor格式\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE,  # 设置批量大小\n",
    "                          num_workers=8,  # 设置数据加载的并行工作线程数\n",
    "                          shuffle=True)  # 是否打乱数据\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,  # 设置批量大小\n",
    "                         num_workers=8,  # 设置数据加载的并行工作线程数\n",
    "                         shuffle=False)  # 不打乱测试数据\n",
    "\n",
    "\n",
    "# 检查数据集\n",
    "for images, labels in train_loader:  # 遍历训练数据加载器中的每个batch\n",
    "    print('图像批次维度:', images.shape)  # 打印图像的维度\n",
    "    print('图像标签维度:', labels.shape)  # 打印标签的维度\n",
    "    break  # 只打印一次数据维度，避免浪费计算资源\n",
    "\n",
    "# 再次检查数据集\n",
    "for images, labels in train_loader:  # 遍历训练数据加载器中的每个batch\n",
    "    print('图像批次维度:', images.shape)  # 打印图像的维度\n",
    "    print('图像标签维度:', labels.shape)  # 打印标签的维度\n",
    "    break  # 只打印一次数据维度，避免浪费计算资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = resnet34(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0196 | Cost: 2.6471\n",
      "Epoch: 001/010 | Batch 0150/0196 | Cost: 1.1742\n",
      "Epoch: 001/010 | Train: 42.820%\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 002/010 | Batch 0000/0196 | Cost: 1.2281\n",
      "Epoch: 002/010 | Batch 0150/0196 | Cost: 1.0295\n",
      "Epoch: 002/010 | Train: 64.700%\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 003/010 | Batch 0000/0196 | Cost: 0.8946\n",
      "Epoch: 003/010 | Batch 0150/0196 | Cost: 0.9104\n",
      "Epoch: 003/010 | Train: 65.626%\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 004/010 | Batch 0000/0196 | Cost: 0.7971\n",
      "Epoch: 004/010 | Batch 0150/0196 | Cost: 0.9031\n",
      "Epoch: 004/010 | Train: 67.766%\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 005/010 | Batch 0000/0196 | Cost: 0.5787\n",
      "Epoch: 005/010 | Batch 0150/0196 | Cost: 0.5928\n",
      "Epoch: 005/010 | Train: 54.922%\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 006/010 | Batch 0000/0196 | Cost: 0.6084\n",
      "Epoch: 006/010 | Batch 0150/0196 | Cost: 0.5809\n",
      "Epoch: 006/010 | Train: 80.326%\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 007/010 | Batch 0000/0196 | Cost: 0.3953\n",
      "Epoch: 007/010 | Batch 0150/0196 | Cost: 0.5537\n",
      "Epoch: 007/010 | Train: 80.608%\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 008/010 | Batch 0000/0196 | Cost: 0.4401\n",
      "Epoch: 008/010 | Batch 0150/0196 | Cost: 0.4132\n",
      "Epoch: 008/010 | Train: 76.746%\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 009/010 | Batch 0000/0196 | Cost: 0.3274\n",
      "Epoch: 009/010 | Batch 0150/0196 | Cost: 0.3597\n",
      "Epoch: 009/010 | Train: 87.440%\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 010/010 | Batch 0000/0196 | Cost: 0.2067\n",
      "Epoch: 010/010 | Batch 0150/0196 | Cost: 0.2963\n",
      "Epoch: 010/010 | Train: 87.644%\n",
      "Time elapsed: 1.15 min\n",
      "Total Training Time: 1.15 min\n",
      "Test accuracy: 69.68%\n",
      "Total Time: 1.16 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0  # 初始化正确预测数和总样本数\n",
    "    for i, (features, targets) in enumerate(data_loader):  # 遍历数据加载器中的每个批次\n",
    "        \n",
    "        features = features.to(device)  # 将输入数据移到指定设备\n",
    "        targets = targets.to(device)  # 将标签数据移到指定设备\n",
    "\n",
    "        logits, probas = model(features)  # 获取模型的预测结果和概率分布\n",
    "        _, predicted_labels = torch.max(probas, 1)  # 获取预测标签\n",
    "        num_examples += targets.size(0)  # 累加样本数量\n",
    "        correct_pred += (predicted_labels == targets).sum()  # 累加正确预测的数量\n",
    "    return correct_pred.float()/num_examples * 100  # 返回准确率\n",
    "\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "for epoch in range(NUM_EPOCHS):  # 遍历所有epoch\n",
    "    \n",
    "    model.train()  # 设置模型为训练模式\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):  # 遍历训练数据加载器中的每个批次\n",
    "        \n",
    "        features = features.to(DEVICE)  # 将输入数据移到指定设备\n",
    "        targets = targets.to(DEVICE)  # 将标签数据移到指定设备\n",
    "            \n",
    "        ### 正向传播和反向传播\n",
    "        logits, probas = model(features)  # 获取模型的预测结果和概率分布\n",
    "        cost = F.cross_entropy(logits, targets)  # 计算交叉熵损失\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        \n",
    "        cost.backward()  # 计算当前的梯度\n",
    "        \n",
    "        ### 更新模型参数\n",
    "        optimizer.step()  # 使用优化器更新模型的参数\n",
    "        \n",
    "        ### 日志记录\n",
    "        if not batch_idx % 150:  # 每150个批次输出一次日志\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   % (epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                      len(train_loader), cost))\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.set_grad_enabled(False):  # 在推理时不计算梯度，节省内存\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))  # 输出当前epoch的训练准确率\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))  # 输出当前epoch已用时间\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))  # 输出总训练时间\n",
    "\n",
    "\n",
    "with torch.set_grad_enabled(False):  # 在推理时不计算梯度，节省内存\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))  # 输出测试集准确率\n",
    "    \n",
    "print('Total Time: %.2f min' % ((time.time() - start_time)/60))  # 输出总耗时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pinned Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像批次尺寸: torch.Size([256, 3, 32, 32])\n",
      "标签批次尺寸: torch.Size([256])\n",
      "图像批次尺寸: torch.Size([256, 3, 32, 32])\n",
      "标签批次尺寸: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### CIFAR-10 数据集\n",
    "##########################\n",
    "\n",
    "# 注意 transforms.ToTensor() 会将输入图像缩放到0-1范围\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True,  # 加载训练集\n",
    "                                 transform=transforms.ToTensor(),  # 转换为Tensor并归一化到0-1\n",
    "                                 download=True)  # 如果数据集不存在则下载\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False,  # 加载测试集\n",
    "                                transform=transforms.ToTensor())  # 转换为Tensor并归一化到0-1\n",
    "\n",
    "\n",
    "# 创建训练数据加载器\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE,  # 每批次的样本数\n",
    "                          pin_memory=True,  # 将数据加载到固定内存中，以便提高数据传输效率\n",
    "                          shuffle=True)  # 打乱数据\n",
    "\n",
    "# 创建测试数据加载器\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,  # 每批次的样本数\n",
    "                         pin_memory=True,  # 将数据加载到固定内存中\n",
    "                         shuffle=False)  # 不打乱数据\n",
    "\n",
    "# 检查数据集\n",
    "for images, labels in train_loader:  # 遍历数据加载器中的一个批次\n",
    "    print('图像批次尺寸:', images.shape)  # 打印图像数据的形状\n",
    "    print('标签批次尺寸:', labels.shape)  # 打印标签数据的形状\n",
    "    break  # 只查看第一个批次\n",
    "\n",
    "# 再次检查数据集\n",
    "for images, labels in train_loader:  # 遍历数据加载器中的一个批次\n",
    "    print('图像批次尺寸:', images.shape)  # 打印图像数据的形状\n",
    "    print('标签批次尺寸:', labels.shape)  # 打印标签数据的形状\n",
    "    break  # 只查看第一个批次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = resnet34(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0196 | Cost: 2.6471\n",
      "Epoch: 001/010 | Batch 0150/0196 | Cost: 1.1742\n",
      "Epoch: 001/010 | Train: 42.820%\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 002/010 | Batch 0000/0196 | Cost: 1.2281\n",
      "Epoch: 002/010 | Batch 0150/0196 | Cost: 1.0295\n",
      "Epoch: 002/010 | Train: 64.700%\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 003/010 | Batch 0000/0196 | Cost: 0.8946\n",
      "Epoch: 003/010 | Batch 0150/0196 | Cost: 0.9104\n",
      "Epoch: 003/010 | Train: 65.626%\n",
      "Time elapsed: 0.36 min\n",
      "Epoch: 004/010 | Batch 0000/0196 | Cost: 0.7971\n",
      "Epoch: 004/010 | Batch 0150/0196 | Cost: 0.9031\n",
      "Epoch: 004/010 | Train: 67.766%\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 005/010 | Batch 0000/0196 | Cost: 0.5787\n",
      "Epoch: 005/010 | Batch 0150/0196 | Cost: 0.5928\n",
      "Epoch: 005/010 | Train: 54.922%\n",
      "Time elapsed: 0.61 min\n",
      "Epoch: 006/010 | Batch 0000/0196 | Cost: 0.6084\n",
      "Epoch: 006/010 | Batch 0150/0196 | Cost: 0.5809\n",
      "Epoch: 006/010 | Train: 80.326%\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 007/010 | Batch 0000/0196 | Cost: 0.3953\n",
      "Epoch: 007/010 | Batch 0150/0196 | Cost: 0.5537\n",
      "Epoch: 007/010 | Train: 80.608%\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 008/010 | Batch 0000/0196 | Cost: 0.4401\n",
      "Epoch: 008/010 | Batch 0150/0196 | Cost: 0.4132\n",
      "Epoch: 008/010 | Train: 76.746%\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 009/010 | Batch 0000/0196 | Cost: 0.3274\n",
      "Epoch: 009/010 | Batch 0150/0196 | Cost: 0.3597\n",
      "Epoch: 009/010 | Train: 87.440%\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 010/010 | Batch 0000/0196 | Cost: 0.2067\n",
      "Epoch: 010/010 | Batch 0150/0196 | Cost: 0.2963\n",
      "Epoch: 010/010 | Train: 87.644%\n",
      "Time elapsed: 1.22 min\n",
      "Total Training Time: 1.22 min\n",
      "测试集准确率: 69.68%\n",
      "Total Time: 1.23 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0  # 初始化正确预测数和样本总数\n",
    "    for i, (features, targets) in enumerate(data_loader):  # 遍历数据加载器中的每个batch\n",
    "            \n",
    "        features = features.to(device)  # 将特征数据传输到指定设备\n",
    "        targets = targets.to(device)  # 将目标标签传输到指定设备\n",
    "\n",
    "        logits, probas = model(features)  # 获取模型的输出，logits是未归一化的logit值，probas是归一化的概率值\n",
    "        _, predicted_labels = torch.max(probas, 1)  # 通过最大概率选择预测标签\n",
    "        num_examples += targets.size(0)  # 累加当前batch的样本数量\n",
    "        correct_pred += (predicted_labels == targets).sum()  # 统计正确预测的数量\n",
    "    return correct_pred.float()/num_examples * 100  # 返回准确率\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "for epoch in range(NUM_EPOCHS):  # 训练多个epoch\n",
    "    \n",
    "    model.train()  # 设置模型为训练模式\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):  # 遍历训练数据加载器中的每个batch\n",
    "        \n",
    "        features = features.to(DEVICE)  # 将特征数据传输到指定设备\n",
    "        targets = targets.to(DEVICE)  # 将目标标签传输到指定设备\n",
    "            \n",
    "        ### 前向传播和反向传播\n",
    "        logits, probas = model(features)  # 获取模型的输出\n",
    "        cost = F.cross_entropy(logits, targets)  # 计算交叉熵损失\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        \n",
    "        cost.backward()  # 反向传播，计算梯度\n",
    "        \n",
    "        ### 更新模型参数\n",
    "        optimizer.step()  # 更新模型的权重参数\n",
    "        \n",
    "        ### 日志记录\n",
    "        if not batch_idx % 150:  # 每150个batch打印一次日志\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.set_grad_enabled(False):  # 在推理过程中禁用梯度计算，节省内存\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))  # 打印训练集准确率\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))  # 打印已经经过的时间\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))  # 打印总训练时间\n",
    "\n",
    "\n",
    "with torch.set_grad_enabled(False):  # 在推理过程中禁用梯度计算，节省内存\n",
    "    print('测试集准确率: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))  # 打印测试集准确率\n",
    "    \n",
    "print('Total Time: %.2f min' % ((time.time() - start_time)/60))  # 打印总时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training time without and with `pin_memory=True`, there doesn't seem to be a speed-up when using page-locked (or \"pinned\") memory -- in fact, pinning the memory even slowed down the training. (I reran the code in the opposite order, i.e., `pin_memory=True` first, and got the same results.)  \n",
    "根据使用`pin_memory=True`与不使用的训练时间，似乎使用页锁定（或“固定”）内存并没有加速——实际上，锁定内存甚至使训练变慢了。（我以相反的顺序重新运行了代码，即先使用`pin_memory=True`，并得到了相同的结果。）  \n",
    "\n",
    "This could be due to the relatively small dataset size, batch size, and hardware configuration that I was using:  \n",
    "这可能是由于我使用的相对较小的数据集大小、批量大小和硬件配置所致："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas     : 2.2.3\n",
      "PIL        : 11.1.0\n",
      "matplotlib : 3.10.1\n",
      "torchvision: 0.21.0+cu126\n",
      "torch      : 2.6.0+cu126\n",
      "numpy      : 1.26.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
