{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEBilEjLj5wY"
   },
   "source": [
    "Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.\n",
    "- Author: Sebastian Raschka\n",
    "- GitHub Repository: https://github.com/rasbt/deeplearning-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1524974472601,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "GOzuY8Yvj5wb",
    "outputId": "c19362ce-f87a-4cc2-84cc-8d7b4b9e6007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.11\n",
      "IPython version      : 9.0.2\n",
      "\n",
      "torch: 2.6.0+cu126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MEu9MiOxj5wk"
   },
   "source": [
    "- Runs on CPU (not recommended here) or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4XmErYj5wm"
   },
   "source": [
    "# Model Zoo -- Convolutional Neural Network (VGG19 Architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the VGG-19 architecture on Cifar10.  \n",
    "\n",
    "\n",
    "Reference for VGG-19:\n",
    "    \n",
    "- Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n",
    "\n",
    "\n",
    "The following table (taken from Simonyan & Zisserman referenced above) summarizes the VGG19 architecture:\n",
    "\n",
    "![](../images/vgg19/vgg19-arch-table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkoGLH_Tj5wn"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.set_per_process_memory_fraction(0.5, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvgJ_0i7j5wt"
   },
   "source": [
    "## Settings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的设备: cuda:0\n",
      "图像批次的维度: torch.Size([256, 3, 32, 32])\n",
      "标签的维度: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### 配置设置\n",
    "##########################\n",
    "\n",
    "# 设备选择：如果有可用的 GPU，则使用 GPU，否则使用 CPU\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('使用的设备:', DEVICE)\n",
    "\n",
    "# 超参数设置\n",
    "random_seed = 1            # 随机种子\n",
    "learning_rate = 0.001      # 学习率\n",
    "num_epochs = 20            # 训练的轮数\n",
    "batch_size = 256           # 每批次的样本数量\n",
    "\n",
    "# 网络架构设置\n",
    "num_features = 784         # 特征数量（例如：MNIST 图像的像素数，28x28=784）\n",
    "num_classes = 10           # 类别数量（对于 CIFAR10 数据集，类别数为 10）\n",
    "\n",
    "##########################\n",
    "### CIFAR10 数据集\n",
    "##########################\n",
    "\n",
    "# 注意：transforms.ToTensor() 会将输入图像的像素值缩放到 0-1 范围内\n",
    "train_dataset = datasets.CIFAR10(root='data',  # 数据存储路径\n",
    "                                 train=True,   # 加载训练集\n",
    "                                 transform=transforms.ToTensor(),  # 进行 ToTensor 变换\n",
    "                                 download=True)  # 下载数据集（如果没有下载的话）\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data',   # 数据存储路径\n",
    "                                train=False,  # 加载测试集\n",
    "                                transform=transforms.ToTensor())  # 进行 ToTensor 变换\n",
    "\n",
    "# 创建训练数据加载器（DataLoader），批量读取训练集数据\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size,  # 每批次的数据量\n",
    "                          shuffle=True)           # 是否打乱数据顺序\n",
    "\n",
    "# 创建测试数据加载器（DataLoader），批量读取测试集数据\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,  # 每批次的数据量\n",
    "                         shuffle=False)          # 不打乱数据顺序\n",
    "\n",
    "# 检查数据集的维度（查看一个批次的数据形状）\n",
    "for images, labels in train_loader:  \n",
    "    print('图像批次的维度:', images.shape)  # 输出图像的维度，通常是 (batch_size, channels, height, width)\n",
    "    print('标签的维度:', labels.shape)       # 输出标签的维度，通常是 (batch_size,)\n",
    "    break  # 只检查第一个批次的数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### 模型定义：VGG19\n",
    "##########################\n",
    "\n",
    "class VGG19(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        # 计算 same padding（保持输出尺寸不变）：\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # 推导出：p = (s(o-1) - w + k)/2\n",
    "\n",
    "        # 第1个卷积块：输入为RGB图像(3通道)，输出为64通道\n",
    "        self.block_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),  # 保持尺寸不变\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),  # 尺寸减半\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # 第2个卷积块：128通道输出\n",
    "        self.block_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # 第3个卷积块：堆叠更多卷积层，输出256通道\n",
    "        self.block_3 = nn.Sequential(        \n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # 第4个卷积块：输出512通道\n",
    "        self.block_4 = nn.Sequential(   \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),   \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # 第5个卷积块：继续保持512通道，确保有4个卷积层\n",
    "        self.block_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),    \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),   \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))             \n",
    "        )\n",
    "        \n",
    "        # 全连接分类器部分（VGG19原始结构中为3层全连接）\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(512, 4096),  # 输入特征数量是 512，输出为 4096\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(4096, num_classes),  # 最后一层输出 num_classes 个类别\n",
    "        )\n",
    "            \n",
    "        # 初始化卷积层和全连接层权重（使用正态分布初始化，均值0，标准差0.05）\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                m.weight.detach().normal_(0, 0.05)  # 初始化卷积层权重\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()  # 将卷积层偏置初始化为0\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                m.weight.detach().normal_(0, 0.05)  # 初始化全连接层权重\n",
    "                m.bias.detach().detach().zero_()  # 将全连接层偏置初始化为0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 前向传播过程，依次经过5个卷积块\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        \n",
    "        # 展平后送入全连接层\n",
    "        logits = self.classifier(x.view(-1, 512))  # 展平为一维向量，送入全连接层\n",
    "        probas = F.softmax(logits, dim=1)  # 使用 softmax 函数计算类别概率\n",
    "\n",
    "        return logits, probas  # 返回 logits 和概率值\n",
    "\n",
    "    \n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# 初始化模型\n",
    "model = VGG19(num_features=num_features,\n",
    "              num_classes=num_classes)\n",
    "\n",
    "# 将模型移动到指定设备（GPU 或 CPU）\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# 使用 Adam 优化器，传入模型的参数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAodboScj5w6"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 0000/0196 | Cost: 1033.0120\n",
      "Epoch: 001/020 | Batch 0050/0196 | Cost: 2.3039\n",
      "Epoch: 001/020 | Batch 0100/0196 | Cost: 2.2875\n",
      "Epoch: 001/020 | Batch 0150/0196 | Cost: 2.2380\n",
      "Epoch: 001/020 | Train: 20.082% |  Loss: 1.939\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 002/020 | Batch 0000/0196 | Cost: 1.9325\n",
      "Epoch: 002/020 | Batch 0050/0196 | Cost: 1.8030\n",
      "Epoch: 002/020 | Batch 0100/0196 | Cost: 1.8392\n",
      "Epoch: 002/020 | Batch 0150/0196 | Cost: 1.6961\n",
      "Epoch: 002/020 | Train: 37.892% |  Loss: 1.620\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 003/020 | Batch 0000/0196 | Cost: 1.6043\n",
      "Epoch: 003/020 | Batch 0050/0196 | Cost: 1.5903\n",
      "Epoch: 003/020 | Batch 0100/0196 | Cost: 1.3592\n",
      "Epoch: 003/020 | Batch 0150/0196 | Cost: 1.4599\n",
      "Epoch: 003/020 | Train: 45.862% |  Loss: 1.451\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 004/020 | Batch 0000/0196 | Cost: 1.4581\n",
      "Epoch: 004/020 | Batch 0050/0196 | Cost: 1.1741\n",
      "Epoch: 004/020 | Batch 0100/0196 | Cost: 1.3137\n",
      "Epoch: 004/020 | Batch 0150/0196 | Cost: 1.1783\n",
      "Epoch: 004/020 | Train: 51.610% |  Loss: 1.318\n",
      "Time elapsed: 1.67 min\n",
      "Epoch: 005/020 | Batch 0000/0196 | Cost: 1.1979\n",
      "Epoch: 005/020 | Batch 0050/0196 | Cost: 1.2043\n",
      "Epoch: 005/020 | Batch 0100/0196 | Cost: 1.1749\n",
      "Epoch: 005/020 | Batch 0150/0196 | Cost: 1.1268\n",
      "Epoch: 005/020 | Train: 52.436% |  Loss: 1.270\n",
      "Time elapsed: 2.09 min\n",
      "Epoch: 006/020 | Batch 0000/0196 | Cost: 1.2755\n",
      "Epoch: 006/020 | Batch 0050/0196 | Cost: 1.1254\n",
      "Epoch: 006/020 | Batch 0100/0196 | Cost: 1.0169\n",
      "Epoch: 006/020 | Batch 0150/0196 | Cost: 1.1005\n",
      "Epoch: 006/020 | Train: 63.400% |  Loss: 1.016\n",
      "Time elapsed: 2.50 min\n",
      "Epoch: 007/020 | Batch 0000/0196 | Cost: 0.9707\n",
      "Epoch: 007/020 | Batch 0050/0196 | Cost: 0.9657\n",
      "Epoch: 007/020 | Batch 0100/0196 | Cost: 0.9883\n",
      "Epoch: 007/020 | Batch 0150/0196 | Cost: 1.0376\n",
      "Epoch: 007/020 | Train: 66.238% |  Loss: 0.949\n",
      "Time elapsed: 2.92 min\n",
      "Epoch: 008/020 | Batch 0000/0196 | Cost: 1.0121\n",
      "Epoch: 008/020 | Batch 0050/0196 | Cost: 0.8246\n",
      "Epoch: 008/020 | Batch 0100/0196 | Cost: 0.9026\n",
      "Epoch: 008/020 | Batch 0150/0196 | Cost: 0.8412\n",
      "Epoch: 008/020 | Train: 66.286% |  Loss: 0.929\n",
      "Time elapsed: 3.34 min\n",
      "Epoch: 009/020 | Batch 0000/0196 | Cost: 1.0241\n",
      "Epoch: 009/020 | Batch 0050/0196 | Cost: 0.8334\n",
      "Epoch: 009/020 | Batch 0100/0196 | Cost: 0.8857\n",
      "Epoch: 009/020 | Batch 0150/0196 | Cost: 0.7908\n",
      "Epoch: 009/020 | Train: 70.848% |  Loss: 0.812\n",
      "Time elapsed: 3.75 min\n",
      "Epoch: 010/020 | Batch 0000/0196 | Cost: 0.8654\n",
      "Epoch: 010/020 | Batch 0050/0196 | Cost: 0.8774\n",
      "Epoch: 010/020 | Batch 0100/0196 | Cost: 0.7525\n",
      "Epoch: 010/020 | Batch 0150/0196 | Cost: 0.7815\n",
      "Epoch: 010/020 | Train: 67.850% |  Loss: 0.909\n",
      "Time elapsed: 4.17 min\n",
      "Epoch: 011/020 | Batch 0000/0196 | Cost: 0.9504\n",
      "Epoch: 011/020 | Batch 0050/0196 | Cost: 0.7351\n",
      "Epoch: 011/020 | Batch 0100/0196 | Cost: 0.8677\n",
      "Epoch: 011/020 | Batch 0150/0196 | Cost: 0.8911\n",
      "Epoch: 011/020 | Train: 76.312% |  Loss: 0.687\n",
      "Time elapsed: 4.58 min\n",
      "Epoch: 012/020 | Batch 0000/0196 | Cost: 0.7438\n",
      "Epoch: 012/020 | Batch 0050/0196 | Cost: 0.6262\n",
      "Epoch: 012/020 | Batch 0100/0196 | Cost: 0.8315\n",
      "Epoch: 012/020 | Batch 0150/0196 | Cost: 0.7595\n",
      "Epoch: 012/020 | Train: 76.942% |  Loss: 0.657\n",
      "Time elapsed: 5.00 min\n",
      "Epoch: 013/020 | Batch 0000/0196 | Cost: 0.7103\n",
      "Epoch: 013/020 | Batch 0050/0196 | Cost: 0.5504\n",
      "Epoch: 013/020 | Batch 0100/0196 | Cost: 0.7403\n",
      "Epoch: 013/020 | Batch 0150/0196 | Cost: 0.7525\n",
      "Epoch: 013/020 | Train: 78.230% |  Loss: 0.611\n",
      "Time elapsed: 5.42 min\n",
      "Epoch: 014/020 | Batch 0000/0196 | Cost: 0.6095\n",
      "Epoch: 014/020 | Batch 0050/0196 | Cost: 0.5947\n",
      "Epoch: 014/020 | Batch 0100/0196 | Cost: 0.6566\n",
      "Epoch: 014/020 | Batch 0150/0196 | Cost: 0.8169\n",
      "Epoch: 014/020 | Train: 77.556% |  Loss: 0.638\n",
      "Time elapsed: 5.83 min\n",
      "Epoch: 015/020 | Batch 0000/0196 | Cost: 0.6847\n",
      "Epoch: 015/020 | Batch 0050/0196 | Cost: 0.4381\n",
      "Epoch: 015/020 | Batch 0100/0196 | Cost: 0.4596\n",
      "Epoch: 015/020 | Batch 0150/0196 | Cost: 0.5485\n",
      "Epoch: 015/020 | Train: 83.630% |  Loss: 0.472\n",
      "Time elapsed: 6.25 min\n",
      "Epoch: 016/020 | Batch 0000/0196 | Cost: 0.4801\n",
      "Epoch: 016/020 | Batch 0050/0196 | Cost: 0.6102\n",
      "Epoch: 016/020 | Batch 0100/0196 | Cost: 0.5939\n",
      "Epoch: 016/020 | Batch 0150/0196 | Cost: 0.7361\n",
      "Epoch: 016/020 | Train: 84.708% |  Loss: 0.439\n",
      "Time elapsed: 6.67 min\n",
      "Epoch: 017/020 | Batch 0000/0196 | Cost: 0.4509\n",
      "Epoch: 017/020 | Batch 0050/0196 | Cost: 0.5445\n",
      "Epoch: 017/020 | Batch 0100/0196 | Cost: 0.3490\n",
      "Epoch: 017/020 | Batch 0150/0196 | Cost: 0.4575\n",
      "Epoch: 017/020 | Train: 85.386% |  Loss: 0.429\n",
      "Time elapsed: 7.08 min\n",
      "Epoch: 018/020 | Batch 0000/0196 | Cost: 0.3878\n",
      "Epoch: 018/020 | Batch 0050/0196 | Cost: 0.4513\n",
      "Epoch: 018/020 | Batch 0100/0196 | Cost: 0.4000\n",
      "Epoch: 018/020 | Batch 0150/0196 | Cost: 0.4949\n",
      "Epoch: 018/020 | Train: 86.244% |  Loss: 0.398\n",
      "Time elapsed: 7.50 min\n",
      "Epoch: 019/020 | Batch 0000/0196 | Cost: 0.3524\n",
      "Epoch: 019/020 | Batch 0050/0196 | Cost: 0.4372\n",
      "Epoch: 019/020 | Batch 0100/0196 | Cost: 0.3724\n",
      "Epoch: 019/020 | Batch 0150/0196 | Cost: 0.4236\n",
      "Epoch: 019/020 | Train: 86.396% |  Loss: 0.387\n",
      "Time elapsed: 7.92 min\n",
      "Epoch: 020/020 | Batch 0000/0196 | Cost: 0.4598\n",
      "Epoch: 020/020 | Batch 0050/0196 | Cost: 0.4190\n",
      "Epoch: 020/020 | Batch 0100/0196 | Cost: 0.4104\n",
      "Epoch: 020/020 | Batch 0150/0196 | Cost: 0.3815\n",
      "Epoch: 020/020 | Train: 88.842% |  Loss: 0.316\n",
      "Time elapsed: 8.33 min\n",
      "Total Training Time: 8.33 min\n"
     ]
    }
   ],
   "source": [
    "# 计算模型在给定数据加载器上的准确率\n",
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()  # 设置模型为评估模式，关闭dropout等\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    # 遍历数据加载器中的每一个批次\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(DEVICE)  # 将特征数据移到设备（GPU 或 CPU）\n",
    "        targets = targets.to(DEVICE)    # 将目标标签移到设备（GPU 或 CPU）\n",
    "\n",
    "        logits, probas = model(features)  # 获取模型的输出 logits 和概率值\n",
    "        _, predicted_labels = torch.max(probas, 1)  # 获取预测的标签，`1` 表示按行选择最大值\n",
    "        \n",
    "        num_examples += targets.size(0)  # 更新总样本数\n",
    "        correct_pred += (predicted_labels == targets).sum()  # 计算预测正确的样本数\n",
    "    \n",
    "    # 返回准确率\n",
    "    return correct_pred.float() / num_examples * 100  # 返回百分比形式的准确率\n",
    "\n",
    "\n",
    "# 计算模型在给定数据加载器上的平均损失\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():  # 在此上下文中，禁用梯度计算，节省内存\n",
    "        # 遍历数据加载器中的每一个批次\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)  # 将特征数据移到设备\n",
    "            targets = targets.to(DEVICE)    # 将目标标签移到设备\n",
    "            logits, probas = model(features)  # 获取模型的输出\n",
    "            loss = F.cross_entropy(logits, targets, reduction='sum')  # 计算交叉熵损失，`sum`表示对所有样本的损失求和\n",
    "            num_examples += targets.size(0)  # 更新样本数\n",
    "            curr_loss += loss  # 累加损失\n",
    "\n",
    "        curr_loss = curr_loss / num_examples  # 计算平均损失\n",
    "        return curr_loss  # 返回平均损失\n",
    "    \n",
    "\n",
    "# 训练过程开始\n",
    "start_time = time.time()  # 记录训练开始时间\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()  # 设置模型为训练模式\n",
    "    # 遍历训练集的每一个批次\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)  # 将特征数据移到设备\n",
    "        targets = targets.to(DEVICE)    # 将目标标签移到设备\n",
    "            \n",
    "        ### 正向传播和反向传播 ###\n",
    "        logits, probas = model(features)  # 获取模型的输出\n",
    "        cost = F.cross_entropy(logits, targets)  # 计算交叉熵损失\n",
    "        optimizer.zero_grad()  # 清除之前计算的梯度\n",
    "        \n",
    "        cost.backward()  # 反向传播计算梯度\n",
    "        \n",
    "        ### 更新模型参数 ###\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        \n",
    "        ### 记录日志 ###\n",
    "        if not batch_idx % 50:  # 每50个批次打印一次日志\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   % (epoch+1, num_epochs, batch_idx, \n",
    "                      len(train_loader), cost))  # 输出当前轮次、批次和损失值\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.set_grad_enabled(False):  # 禁用梯度计算，节省内存\n",
    "        # 输出训练集的准确率和损失\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% |  Loss: %.3f' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader),  # 计算并输出训练集的准确率\n",
    "              compute_epoch_loss(model, train_loader)))  # 计算并输出训练集的平均损失\n",
    "\n",
    "\n",
    "    # 输出每个epoch的时间\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
    "    \n",
    "# 输出总的训练时间\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paaeEQHQj5xC"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6514,
     "status": "ok",
     "timestamp": 1524976895054,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "gzQMWKq5j5xE",
    "outputId": "de7dc005-5eeb-4177-9f9f-d9b5d1358db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 72.78%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # 禁用梯度计算，节省内存\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      : 2.6.0+cu126\n",
      "torchvision: 0.21.0+cu126\n",
      "numpy      : 1.26.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
